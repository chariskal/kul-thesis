{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "# input image\n",
    "LABELS_file = 'kvasir-labels.json'\n",
    "image_file = '/home/charis/kul-thesis/kvasir-dataset-v2/images/polyps/cju0qkwl35piu0993l0dewei2.jpg'\n",
    "\n",
    "# networks such as googlenet, resnet, densenet already use global average pooling at the end, so CAM could be used directly.\n",
    "model_id = 2\n",
    "if model_id == 1:\n",
    "    net = models.squeezenet1_1(pretrained=True)\n",
    "    finalconv_name = 'features' # this is the last conv layer of the network\n",
    "elif model_id == 2:\n",
    "    net = models.resnet50(pretrained=True)\n",
    "    finalconv_name = 'layer4'\n",
    "elif model_id == 3:\n",
    "    net = models.densenet161(pretrained=True)\n",
    "    finalconv_name = 'features'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# load the imagenet category list\n",
    "with open(LABELS_file) as f:\n",
    "    classes = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures = net.fc.in_features\n",
    "# loop over the modules of the net and set the parameters of\n",
    "# batch normalization modules as not trainable\n",
    "for module, param in zip(net.modules(), net.parameters()):\n",
    "\tif isinstance(module, nn.BatchNorm2d):\n",
    "\t\tparam.requires_grad = False\n",
    "# define the network head and attach it to the net\n",
    "headnet = nn.Sequential(\n",
    "\tnn.Linear(numFeatures, len(classes)),\n",
    ")\n",
    "net.fc = headnet\n",
    "# append a new classification top to our feature extractor and pop it\n",
    "# on to the current device\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/charis/kul-thesis/kvasir-dataset-v2/\"\n",
    "\n",
    "VAL_SPLIT = 0.1\n",
    "TRAIN = os.path.join(BASE_PATH, \"images\")\n",
    "TRAIN_LABELS = os.path.join(BASE_PATH, \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify ImageNet mean and standard deviation and image size\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "IMAGE_SIZE = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_EXTRACTION_BATCH_SIZE = 256\n",
    "FINETUNE_BATCH_SIZE = 2\n",
    "PRED_BATCH_SIZE = 8\n",
    "EPOCHS = 40\n",
    "LR = 0.001\n",
    "LR_FINETUNE = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# get the dataloader (Note: without data augmentation)\n",
    "def get_loader(img_root, label_root, batch_size=FINETUNE_BATCH_SIZE, mode='train', num_thread=os.cpu_count(), pin=True):\n",
    "    if mode == 'train':\n",
    "        # define augmentation pipelines\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(IMAGE_SIZE),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "        t_transform = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "        dataset = datasets.ImageFolder(root=img_root, transform=transform)\n",
    "        data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=num_thread,\n",
    "                                      pin_memory=pin)\n",
    "        return dataset, data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_loader = get_loader(img_root=TRAIN, label_root=TRAIN_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize loss function and optimizer (notice that we are only\n",
    "# providing the parameters of the classification top to our optimizer)\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.fc.parameters(), lr=LR)\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(train_dataset) // FEATURE_EXTRACTION_BATCH_SIZE\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"train_acc\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e740e119f549d980efb3db466e4346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/40\n",
      "Train loss: 212.938339, Train accuracy: 0.3974\n",
      "[INFO] EPOCH: 2/40\n",
      "Train loss: 189.209488, Train accuracy: 0.4819\n",
      "[INFO] EPOCH: 3/40\n",
      "Train loss: 184.525726, Train accuracy: 0.5068\n",
      "[INFO] EPOCH: 4/40\n",
      "Train loss: 180.177063, Train accuracy: 0.5201\n",
      "[INFO] EPOCH: 5/40\n",
      "Train loss: 179.459869, Train accuracy: 0.5249\n",
      "[INFO] EPOCH: 6/40\n",
      "Train loss: 175.062347, Train accuracy: 0.5346\n",
      "[INFO] EPOCH: 7/40\n",
      "Train loss: 179.420853, Train accuracy: 0.5284\n",
      "[INFO] EPOCH: 8/40\n",
      "Train loss: 173.692001, Train accuracy: 0.5415\n",
      "[INFO] EPOCH: 9/40\n",
      "Train loss: 174.105072, Train accuracy: 0.5460\n",
      "[INFO] EPOCH: 10/40\n",
      "Train loss: 174.438736, Train accuracy: 0.5470\n",
      "[INFO] EPOCH: 11/40\n",
      "Train loss: 170.241516, Train accuracy: 0.5633\n",
      "[INFO] EPOCH: 12/40\n",
      "Train loss: 169.548721, Train accuracy: 0.5653\n",
      "[INFO] EPOCH: 13/40\n",
      "Train loss: 168.154709, Train accuracy: 0.5650\n",
      "[INFO] EPOCH: 14/40\n",
      "Train loss: 169.476578, Train accuracy: 0.5553\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training the network...\")\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "net = net.to(device)\n",
    "\n",
    "for e in tqdm(range(EPOCHS)):\n",
    "\tnet.train()\n",
    "\ttotalTrainLoss = 0\n",
    "\ttrainCorrect = 0\n",
    "\tfor (i, (x, y)) in enumerate(train_loader):\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\tpred = net(x)\n",
    "\t\tloss = lossFunc(pred, y)\n",
    "\t\tloss.backward()\n",
    "\t\t# check if we are updating the net parameters and if so\n",
    "\t\t# update them, and zero out the previously accumulated gradients\n",
    "\t\tif (i + 2) % 2 == 0:\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "\n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\t# calculate the training and validation accuracy\n",
    "\ttrainCorrect = trainCorrect / len(train_dataset)\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"train_acc\"].append(trainCorrect)\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, trainCorrect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/home/charis/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:301180)",
      "at w.execute (/home/charis/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300551)",
      "at w.start (/home/charis/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:296215)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/charis/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310950)",
      "at async t.CellExecutionQueue.start (/home/charis/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "trainCorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook the feature extractor\n",
    "net = net.cpu()\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224,224)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test image\n",
    "img_pil = Image.open(image_file)\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable)\n",
    "\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.numpy()\n",
    "idx = idx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.370 -> ulterative-colitis\n",
      "0.184 -> dyed-lifted-polyps\n",
      "0.156 -> esophagitis\n",
      "0.075 -> normal-cecum\n",
      "0.069 -> normal-z-line\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output CAM.jpg for the top1 prediction: ulterative-colitis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "img = cv2.imread(image_file)\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM.jpg', result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a277b6c68689dd8180022a35a6162240ca133d608bcf2ca16ace5cd7494cfe97"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('mai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
