{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_seg.ipynb","provenance":[],"collapsed_sections":["5m6QvdGfPl_K","iJHhjLUarb6r","JhIIsxNpdesp"],"mount_file_id":"1tgB8enYqqyMCQHSiUWseoOdE_04VJV2y","authorship_tag":"ABX9TyOVKXesd0APGWL6DZU8nbMv"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K1fhZPM3P06m"},"source":["# Pre-requisites  \n","- download dataset.zip,  \n","- unzip dataset,    \n","- mount drive,  "]},{"cell_type":"code","metadata":{"id":"AFOXQ7FecMwO"},"source":["# Package installations\n","from IPython.utils import io\n","with io.capture_output() as captured:\n","    !pip install gdown\n","    # !pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n","    !pip install psutil\n","    !pip install neptune-client\n","    !pip install neptune-client neptune-tensorboard\n","    !pip install neptune-contrib\n","    !pip install imgaug==0.2.6\n","    !pip3 install --upgrade Pillow\n","    !pip install mxnet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Pv8I7OzfSUN"},"source":["# Download VOC2012\n","# !gdown https://drive.google.com/uc?id=1PDTEuTnWJZNWogxYdqYGOlEZHK8dYET9\n","\n","# Download custom Kvasir-v2\n","# ! gdown https://drive.google.com/uc?id=1WG5F7VVQe6mNupR1LLirkG_Apx6Bup5J"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dFUSDDvsoyiD"},"source":["# with io.capture_output() as captured:\n","  # !unzip -q VOC2012.zip\n","#   !unzip -q Kvasir-v2.zip\n","  # !rm -rf VOC2012.zip\n","#   !rm -rf Kvasir-v2.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UhupCLMNOeR6"},"source":["import neptune\n","from neptunecontrib.monitoring.keras import NeptuneMonitor\n","from IPython.utils import io\n","# connect run to project\n","NEPTUNE_TOKEN = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhMGUxMmQ1NC00ZDU4LTQ4ZGYtOWJjOC0xYTJkYjJmYmJiZDMifQ=='\n","run = neptune.init(project_qualified_name='ch.kalavritinos/OAA', api_token=NEPTUNE_TOKEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcJiLU-mzqnh"},"source":["from IPython.utils import io"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8PiwVHzFpZb4","executionInfo":{"status":"ok","timestamp":1630919649102,"user_tz":-180,"elapsed":36,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"c4d5422d-b159-430e-864e-edcb89aa40ff"},"source":["with io.capture_output() as captured:\n","  from google.colab import drive \n","  drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/MAI/thesis/source"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MAI/thesis/source\n"]}]},{"cell_type":"code","metadata":{"id":"s7lmdumJACCK"},"source":["# %cd /content/drive/MyDrive/MAI/thesis/source/OAA/results_voc/\n","# !ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z44AHk_GBCFo"},"source":["# !zip -r -q exp2.zip exp2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-p3UTWWGXkJu"},"source":["# !gdown https://drive.google.com/drive/folders/uc?id=1P491c2_U7q6oYvgMR4bcVsoHVcsnGYA0\n","# !unzip -q SEAM_model.zip\n","# !rm -rf SEAM_model.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwdc9096kMdG"},"source":["# OAA segmentation train\n","Imports"]},{"cell_type":"markdown","metadata":{"id":"FzSrMVV1ot0s"},"source":["Change directory to OAA-PyTorch\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7bpdwddmhpl","executionInfo":{"status":"ok","timestamp":1630919649106,"user_tz":-180,"elapsed":21,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"d06ec0f4-30e0-4998-df79-d613008abe42"},"source":["%cd OAA/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MAI/thesis/source/OAA\n"]}]},{"cell_type":"code","metadata":{"id":"z5k_Vu-22WaV"},"source":["from torchvision import transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-YtUHpsr7ki","executionInfo":{"status":"ok","timestamp":1630919649906,"user_tz":-180,"elapsed":400,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"b384f73e-2957-4d4b-b0ab-01cb080fbc23"},"source":["import sys\n","sys.path.append('scripts')\n","sys.path.append('utils')\n","sys.path.append('models')\n","import numpy as np\n","import torch\n","import random\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import voc12.data\n","from utils import pyutils, imutils, torchutils\n","import argparse\n","import importlib\n","import neptune\n","import os\n","\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"38O6PRg61o70","executionInfo":{"status":"ok","timestamp":1630919649908,"user_tz":-180,"elapsed":15,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"61638e19-c75d-476b-918f-cb98dc0488e3"},"source":["import PIL\n","PIL.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'8.3.2'"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"eLOOrO2I1Zgr"},"source":["# PARAMETERS"]},{"cell_type":"code","metadata":{"id":"DN5wuJxuW77d"},"source":["root_dir = 'OAA'\n","data_root = '/content/drive/MyDrive/MAI/thesis/source/kvasir-dataset-v2'   # dataset directory\n","train_list = 'kvasirv2/train.txt'               # list of train images\n","test_list = 'kvasirv2/val.txt'                  # list of val images\n","snapshot_dir = 'checkpoints/train/exp5/'        # where to save models\n","att_dir = './results_kvasir/exp5/attention/'    # where to save attentions\n","dataset = 'kvasir'                              # dataset used\n","\n","\n","epoch = 14\n","lr = 0.001\n","batch_size = 18\n","input_size = 256\n","disp_interval = 100\n","num_classes = 8\n","num_workers = 2\n","weight_decay = 0.0005\n","decay_points = '5,10'\n","\n","crop_size = 224         # for cropping images\n","threshold = 0.6         # for probabilities\n","disp_interval = 100     # display interval\n","resume  = False         # resume training \n","global_counter = 0      \n","current_epoch = 0       # number of current epoch for resuming"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DGeIk8ECqAdN"},"source":["root_dir = 'OAA'\n","data_root = '/content/drive/MyDrive/MAI/thesis/source/VOCdevkit/VOC2012'      # dataset directory\n","# data_root = '/content/VOC2012'                                              # dataset directory\n","\n","train_list = 'voc12/train_aug.txt'                # list of train images\n","test_list = 'voc12/val_oaa.txt'                   # list of val images\n","snapshot_dir = 'checkpoints/train_aff/exp2'       # where to save models\n","# att_dir = './results_voc/exp2/attention/'       # where to save attentions\n","dataset = 'voc'                                   # dataset used\n","\n","\n","epoch = 8\n","lr = 0.01\n","batch_size = 16\n","input_size = 256\n","disp_interval = 100\n","num_classes = 20\n","num_workers = 2\n","weight_decay = 0.0005\n","decay_points = '5,10'\n","\n","session_name = \"resnet38_aff\"\n","crop_size = 224         # for cropping images\n","threshold = 0.6         # for probabilities\n","disp_interval = 100     # display interval\n","resume  = True          # resume training \n","global_counter = 0      \n","current_epoch = 0       # number of current epoch for resuming"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZaX7WbiVj18"},"source":["weights = '/content/drive/MyDrive/MAI/thesis/source/pretrained_models/ilsvrc_cls.params'\n","# For Kvasir\n","la_crf_dir = 'results_kvasir/exp8/results_crf_4.0'\n","ha_crf_dir = 'results_kvasir/exp8/results_crf_24.0'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWboKWtYhhEV"},"source":["# For VOC2012\n","la_crf_dir = 'results_voc/exp2/results_crf_4.0'\n","ha_crf_dir = 'results_voc/exp2/results_crf_24.0'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5m6QvdGfPl_K"},"source":["# data.py  \n","for Kvasir-v2"]},{"cell_type":"code","metadata":{"id":"R57oDOTLDkj7"},"source":["# IMG_FOLDER_NAME = \"polyps/\"\n","# ANNOT_FOLDER_NAME = \"polyps/masks\"\n","\n","# CAT_LIST = ['dyed-lifted-polyp',\n","#             'dyed-resection-margins',\n","#             'esophagitis',\n","#             'normal-cecum',\n","#             'normal-pylorus',\n","#             'normal-z-line', \n","#             'polyps',\n","#             'ulcerative-colitis']\n","\n","# CAT_NAME_TO_NUM = dict(zip(CAT_LIST,range(len(CAT_LIST))))\n","\n","\n","# def load_image_label_list_from_npy(img_name_list):\n","#     cls_labels_dict = np.load('kvasirv2/cls_labels.npy', allow_pickle=True).item()\n","#     # print(cls_labels_dict)\n","#     return [cls_labels_dict[img_name] for img_name in img_name_list]\n","\n","# def get_img_path(img_name, dataset_root):\n","#     return os.path.join(dataset_root, img_name)\n","\n","# def load_img_name_list(dataset_path):\n","#     img_gt_name_list = open(dataset_path).read().splitlines()\n","#     img_name_list = [img_gt_name.split(' ')[0][-40:-4] for img_gt_name in img_gt_name_list]\n","#     folder_paths_list = [img_gt_name.split(' ')[0] for img_gt_name in img_gt_name_list]\n","#     return img_name_list, folder_paths_list\n","\n","# def load_label_list(dataset_path):\n","#     zero_array = np.zeros(8, dtype=np.float32)\n","#     list_of_arrays = []\n","#     for i in range(8):\n","#         z = np.zeros(8, dtype=np.float32)\n","#         z[i]=1.0\n","#         list_of_arrays.append(z)\n","#     #print(list_of_arrays)\n","#     img_name_list = open(dataset_path).read().splitlines()\n","#     label_list = [list_of_arrays[int(img_name[-1:])-1] for img_name in img_name_list]\n","#     return label_list\n","\n","# class KvasirImageDataset(Dataset):\n","#     def __init__(self, img_name_list_path, dataset_root, transform=None):\n","#         self.img_name_list, self.folder_paths_list = load_img_name_list(img_name_list_path)\n","#         self.dataset_root = dataset_root\n","#         self.transform = transform\n","\n","#     def __len__(self):\n","#         return len(self.img_name_list)\n","\n","#     def __getitem__(self, idx):\n","#         name = self.img_name_list[idx]\n","#         path = self.folder_paths_list[idx]\n","#         #print(get_img_path(path, self.dataset_root))\n","#         img = PIL.Image.open(get_img_path(path, self.dataset_root)).convert(\"RGB\")\n","#         # img = torch.from_numpy(np.array(img))\n","\n","#         if self.transform:\n","#             img = self.transform(img)\n","#         return name, img\n","\n","# class KvasirClsDataset(KvasirImageDataset):           # inherit init from previous class\n","#     def __init__(self, img_name_list_path, dataset_root, transform=None):\n","#         super().__init__(img_name_list_path, dataset_root, transform)\n","#         self.label_list = load_image_label_list_from_npy(self.img_name_list)        # get list from .npy file\n","#         #self.label_list = load_image_label_list_from_xml(self.img_name_list, self.dataset_root)\n","\n","#     def __getitem__(self, idx):\n","#         name, img = super().__getitem__(idx)\n","#         label = torch.from_numpy(self.label_list[idx])\n","#         return name, img, label\n","\n","# class KvasirClsDatasetMSF(KvasirClsDataset):\n","#     def __init__(self, img_name_list_path, data_root, scales, inter_transform=None, unit=1):\n","#         super().__init__(img_name_list_path, data_root, transform=None)\n","#         self.scales = scales\n","#         self.unit = unit\n","#         self.inter_transform = inter_transform\n","\n","#     def __getitem__(self, idx):\n","#         name, img, label = super().__getitem__(idx)\n","#         rounded_size = (int(round(img.size[0]/self.unit)*self.unit), int(round(img.size[1]/self.unit)*self.unit))\n","\n","#         ms_img_list = []\n","#         for s in self.scales:\n","#             target_size = (round(rounded_size[0]*s),\n","#                            round(rounded_size[1]*s))\n","#             s_img = img.resize(target_size, resample=PIL.Image.CUBIC)\n","#             ms_img_list.append(s_img)\n","\n","#         if self.inter_transform:\n","#             for i in range(len(ms_img_list)):\n","#                 ms_img_list[i] = self.inter_transform(ms_img_list[i])\n","\n","#         msf_img_list = []\n","#         for i in range(len(ms_img_list)):\n","#             msf_img_list.append(ms_img_list[i])\n","#             msf_img_list.append(np.flip(ms_img_list[i], -1).copy())\n","#         return name, msf_img_list, label\n","\n","# class KvasirClsDatasetMS(KvasirClsDataset):\n","#     def __init__(self, img_name_list_path, dataset_root, scales, inter_transform=None, unit=1):\n","#         super().__init__(img_name_list_path, dataset_root, transform=None)\n","#         self.scales = scales\n","#         self.unit = unit\n","#         self.inter_transform = inter_transform\n","\n","#     def __getitem__(self, idx):\n","#         name, img, label = super().__getitem__(idx)\n","#         rounded_size = (int(round(img.size[0]/self.unit)*self.unit), int(round(img.size[1]/self.unit)*self.unit))\n","#         ms_img_list = []\n","#         for s in self.scales:\n","#             target_size = (round(rounded_size[0]*s),\n","#                            round(rounded_size[1]*s))\n","#             s_img = img.resize(target_size, resample=PIL.Image.CUBIC)\n","#             ms_img_list.append(s_img)\n","\n","#         if self.inter_transform:\n","#             for i in range(len(ms_img_list)):\n","#                 ms_img_list[i] = self.inter_transform(ms_img_list[i])\n","\n","#         return name, ms_img_list, label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJHhjLUarb6r"},"source":["# Losses"]},{"cell_type":"code","metadata":{"id":"gqGPrPSprep2"},"source":["import torch.nn as nn\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        intersection = (inputs * targets).sum()\n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","        return 1 - dice\n","\n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        intersection = (inputs * targets).sum()\n","        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","        return Dice_BCE\n","\n","class IoULoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(IoULoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        #intersection is equivalent to True Positive count\n","        #union is the mutually inclusive area of all labels & predictions\n","        intersection = (inputs * targets).sum()\n","        total = (inputs + targets).sum()\n","        union = total - intersection\n","        IoU = (intersection + smooth)/(union + smooth)\n","\n","        return -IoU\n","\n","class IoUBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(IoUBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        #intersection is equivalent to True Positive count\n","        #union is the mutually inclusive area of all labels & predictions\n","        intersection = (inputs * targets).sum()\n","        total = (inputs + targets).sum()\n","        union = total - intersection\n","        IoU = - (intersection + smooth)/(union + smooth)\n","\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        IoU_BCE = BCE + IoU\n","\n","        return IoU_BCE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JhIIsxNpdesp"},"source":["# imutils.py\n"]},{"cell_type":"code","metadata":{"id":"07j_xMYYdAVf"},"source":["import PIL.Image\n","import random\n","import numpy as np\n","\n","\"\"\"Modified classes to include saliency maps as well\n","        Classes Included:\n","            RandomResizeLong()\n","            RandomCrop()\n","            AvgPool2d()\n","            RandomHorizontalFlip()\n","            CenterCrop()\n","            RescaleNearest()\n","\n","        Functions included: \n","            get_random_crop_box()\n","            crop_with_box()\n","            random_crop()\n","            HWC_to_CHW()\n","            crf_inference()\n","            bb_IOU()\n","            large_rect()\n","\n","\"\"\"\n","class RandomResizeLong():\n","    \"\"\"Resizes images from dataset using PIL to random shape between min_long and max_long\"\"\"\n","\n","    def __init__(self, min_long, max_long):\n","        self.min_long = min_long\n","        self.max_long = max_long\n","\n","    def __call__(self, img, sal=None):\n","        target_long = random.randint(self.min_long, self.max_long)\n","        w, h = img.size\n","\n","        if w < h:\n","            target_shape = (int(round(w * target_long / h)), target_long)\n","        else:\n","            target_shape = (target_long, int(round(h * target_long / w)))\n","\n","        img = img.resize(target_shape, resample=PIL.Image.CUBIC)\n","        if sal:\n","           sal = sal.resize(target_shape, resample=PIL.Image.CUBIC)\n","           return img, sal\n","        return img\n","\n","\n","class RandomCrop():\n","    \"\"\"Randomly crops images\"\"\"\n","    def __init__(self, cropsize):\n","        self.cropsize = cropsize\n","\n","    def __call__(self, imgarr, sal=None):\n","        print(np.shape(imgarr) )\n","        # h, w = np.shape(imgarr)                  # height, width\n","        w, h = img.size\n","        ch = min(self.cropsize, h)\n","        cw = min(self.cropsize, w)\n","\n","        w_space = w - self.cropsize\n","        h_space = h - self.cropsize\n","\n","        if w_space > 0:\n","            cont_left = 0\n","            img_left = random.randrange(w_space+1)\n","        else:\n","            cont_left = random.randrange(-w_space+1)\n","            img_left = 0\n","\n","        if h_space > 0:\n","            cont_top = 0\n","            img_top = random.randrange(h_space+1)\n","        else:\n","            cont_top = random.randrange(-h_space+1)\n","            img_top = 0\n","\n","        container = np.zeros((self.cropsize, self.cropsize, np.shape(imgarr)[-1]), np.float32)\n","        container[cont_top:cont_top+ch, cont_left:cont_left+cw] = \\\n","            imgarr[img_top:img_top+ch, img_left:img_left+cw]\n","        if sal is not None:\n","            container_sal = np.zeros((self.cropsize, self.cropsize,1), np.float32)\n","            container_sal[cont_top:cont_top+ch, cont_left:cont_left+cw,0] = \\\n","                sal[img_top:img_top+ch, img_left:img_left+cw]\n","            return container, container_sal\n","\n","        return container\n","\n","def get_random_crop_box(imgsize, cropsize):\n","    h, w = imgsize\n","\n","    ch = min(cropsize, h)\n","    cw = min(cropsize, w)\n","\n","    w_space = w - cropsize\n","    h_space = h - cropsize\n","\n","    if w_space > 0:\n","        cont_left = 0\n","        img_left = random.randrange(w_space + 1)\n","    else:\n","        cont_left = random.randrange(-w_space + 1)\n","        img_left = 0\n","\n","    if h_space > 0:\n","        cont_top = 0\n","        img_top = random.randrange(h_space + 1)\n","    else:\n","        cont_top = random.randrange(-h_space + 1)\n","        img_top = 0\n","\n","    return cont_top, cont_top+ch, cont_left, cont_left+cw, img_top, img_top+ch, img_left, img_left+cw\n","\n","def crop_with_box(img, box):\n","    if len(img.shape) == 3:\n","        img_cont = np.zeros((max(box[1]-box[0], box[4]-box[5]), max(box[3]-box[2], box[7]-box[6]), img.shape[-1]), dtype=img.dtype)\n","    else:\n","        img_cont = np.zeros((max(box[1] - box[0], box[4] - box[5]), max(box[3] - box[2], box[7] - box[6])), dtype=img.dtype)\n","    img_cont[box[0]:box[1], box[2]:box[3]] = img[box[4]:box[5], box[6]:box[7]]\n","    return img_cont\n","\n","\n","def random_crop(images, cropsize, fills):\n","    if isinstance(images[0], PIL.Image.Image):\n","        imgsize = images[0].size[::-1]\n","    else:\n","        imgsize = images[0].shape[:2]\n","    box = get_random_crop_box(imgsize, cropsize)\n","\n","    new_images = []\n","    for img, f in zip(images, fills):\n","\n","        if isinstance(img, PIL.Image.Image):\n","            img = img.crop((box[6], box[4], box[7], box[5]))\n","            cont = PIL.Image.new(img.mode, (cropsize, cropsize))\n","            cont.paste(img, (box[2], box[0]))\n","            new_images.append(cont)\n","\n","        else:\n","            if len(img.shape) == 3:\n","                cont = np.ones((cropsize, cropsize, img.shape[2]), img.dtype)*f\n","            else:\n","                cont = np.ones((cropsize, cropsize), img.dtype)*f\n","            cont[box[0]:box[1], box[2]:box[3]] = img[box[4]:box[5], box[6]:box[7]]\n","            new_images.append(cont)\n","\n","    return new_images\n","\n","\n","class AvgPool2d():\n","    \"\"\"Average pooling on 2d\"\"\"\n","    def __init__(self, ksize):\n","        self.ksize = ksize\n","\n","    def __call__(self, img):\n","        import skimage.measure\n","\n","        return skimage.measure.block_reduce(img, (self.ksize, self.ksize, 1), np.mean)\n","\n","\n","class RandomHorizontalFlip():\n","    \"\"\"Randomly mirror images\"\"\"\n","    def __init__(self):\n","        return\n","\n","    def __call__(self, img, sal=None):\n","        if bool(random.getrandbits(1)):\n","            #img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n","            img = np.fliplr(img).copy()\n","            if sal:\n","                #sal = sal.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n","                sal = np.fliplr(sal).copy()\n","                return img, sal \n","            return img\n","        else:\n","            if sal:\n","                return img, sal\n","            return img\n","\n","\n","class CenterCrop():\n","    \"\"\"Crop images centered\"\"\"\n","    def __init__(self, cropsize, default_value=0):\n","        self.cropsize = cropsize\n","        self.default_value = default_value\n","\n","    def __call__(self, npimg):\n","        h, w = npimg.shape[:2]\n","\n","        ch = min(self.cropsize, h)\n","        cw = min(self.cropsize, w)\n","\n","        sh = h - self.cropsize\n","        sw = w - self.cropsize\n","\n","        if sw > 0:\n","            cont_left = 0\n","            img_left = int(round(sw / 2))\n","        else:\n","            cont_left = int(round(-sw / 2))\n","            img_left = 0\n","\n","        if sh > 0:\n","            cont_top = 0\n","            img_top = int(round(sh / 2))\n","        else:\n","            cont_top = int(round(-sh / 2))\n","            img_top = 0\n","\n","        if len(npimg.shape) == 2:\n","            container = np.ones((self.cropsize, self.cropsize), npimg.dtype)*self.default_value\n","        else:\n","            container = np.ones((self.cropsize, self.cropsize, npimg.shape[2]), npimg.dtype)*self.default_value\n","\n","        container[cont_top:cont_top+ch, cont_left:cont_left+cw] = \\\n","            npimg[img_top:img_top+ch, img_left:img_left+cw]\n","\n","        return container\n","\n","\n","def HWC_to_CHW(tensor, sal=False):\n","    if sal:\n","        tensor = np.expand_dims(tensor, axis=0)\n","    else:\n","        tensor = np.transpose(tensor, (2, 0, 1))\n","    return tensor\n","\n","\n","class RescaleNearest():\n","    \"\"\"Resize image using nearest neighmor interpolation\"\"\"\n","    def __init__(self, scale):\n","        self.scale = scale\n","\n","    def __call__(self, npimg):\n","        import cv2\n","        return cv2.resize(npimg, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n","\n","\n","def bb_IOU(boxA, boxB):\n","    \"\"\"COmputes IOU based on bounding boxes (predicted and ground truth)\"\"\"\n","    boxA = [float(aa) for aa in boxA]\n","    boxB = [float(bb) for bb in boxB]\n","\n","    xA = max(boxA[0], boxB[0])\n","    yA = max(boxA[1], boxB[1])\n","    xB = min(boxA[2], boxB[2])\n","    yB = min(boxA[3], boxB[3])\n","    \n","    if xA >= xB or yA >= yB:\n","        return 0, 0\n","    # compute the area of intersection rectangle\n","    interArea = (xB - xA + 1) * (yB - yA + 1)\n","\n","    # compute the area of both the prediction and ground-truth rectangles\n","    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n","    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n","\n","    # compute the intersection over union by taking the intersection\n","    # area and dividing it by the sum of prediction + ground-truth\n","    # areas - the interesection area\n","    iou = interArea / float(boxAArea + boxBArea - interArea)\n","    recall = interArea / float(boxAArea)\n","    return iou, recall\n","\n","def large_rect(rect):\n","    \"\"\"find largest rectangles\"\"\"\n","    large_area = 0\n","    target = 0\n","    for i in range(len(rect)):\n","        area = rect[i][2]*rect[i][3]\n","        if large_area < area:\n","            large_area = area\n","            target = i\n","\n","    x = rect[target][0]\n","    y = rect[target][1]\n","    w = rect[target][2]\n","    h = rect[target][3]\n","\n","    return x, y, w, h\n","\n","\n","def crf_inference(img, probs, t=10, scale_factor=1, labels=21):\n","    import pydensecrf.densecrf as dcrf\n","    from pydensecrf.utils import unary_from_softmax\n","\n","    h, w = img.shape[:2]\n","    n_labels = labels\n","\n","    d = dcrf.DenseCRF2D(w, h, n_labels)\n","\n","    unary = unary_from_softmax(probs)\n","    unary = np.ascontiguousarray(unary)\n","    unary = unary.reshape((n_labels,-1))\n","    d.setUnaryEnergy(unary)\n","    d.addPairwiseGaussian(sxy=3/scale_factor, compat=3)\n","    d.addPairwiseBilateral(sxy=80/scale_factor, srgb=13, rgbim=np.copy(img), compat=10)\n","    Q = d.inference(t)\n","\n","    return np.array(Q).reshape((n_labels, h, w))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P7Hv-IM4PqZI"},"source":["# train.py"]},{"cell_type":"code","metadata":{"id":"Hot3Bb5YKdve"},"source":["def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):     # save points during training\n","    \"\"\"Function for saving checkpoints\"\"\"\n","    savepath = os.path.join(snapshot_dir, filename)\n","    torch.save(state, savepath)\n","    print(f\"Model saved to {savepath}\")\n","    \n","\n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    from collections import OrderedDict\n","    new_state_dict = OrderedDict()\n","    for k, v in checkpoint['state_dict'].items():\n","        name = k[7:]                                  # remove `module.`\n","        new_state_dict[name] = v\n","    \n","    model.load_state_dict(new_state_dict)\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    print('weights loaded!')\n","    return model, optimizer, checkpoint['epoch']+1, checkpoint['global_counter']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WQaVYZcNdjUP"},"source":["# Train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukUJFV5L39jH","executionInfo":{"status":"ok","timestamp":1630921998283,"user_tz":-180,"elapsed":403,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"4564bbcc-8d16-4150-d8c6-c411eaad054d"},"source":["torch.cuda.device_count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"Z1tb3vcUvlpq"},"source":["from utils.LoadData import train_data_loader\n","from torch.optim.lr_scheduler import MultiplicativeLR\n","\n","def worker_init_fn(worker_id):\n","        np.random.seed(1 + worker_id)\n","\n","def train(input_size = 224,\n","          disp_interval = 50,\n","          num_classes = 20,\n","          num_workers = 2,\n","          threshold = 0.6,          # for probabilities\n","          resume  = False,          # resume training \n","          current_epoch = 0,\n","          batch_size = 16,\n","          global_counter = 0,\n","          momentum = 0.9,\n","          network=\"network.resnet38_aff\"):\n","\n","    if not os.path.exists(snapshot_dir):\n","        os.makedirs(snapshot_dir)\n","\n","    model = getattr(importlib.import_module(network), 'Net')()\n","    current_epoch = current_epoch\n","    train_dataset = voc12.data.VOC12AffDataset(train_list, label_la_dir=la_crf_dir, label_ha_dir=ha_crf_dir,\n","                                               voc12_root=data_root, cropsize=crop_size, radius=5,\n","                                                joint_transform_list=[\n","                                                    None,\n","                                                    None,\n","                                                    imutils.RandomCrop(crop_size),\n","                                                    imutils.RandomHorizontalFlip()\n","                                                ],\n","                                                img_transform_list=[\n","                                                    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n","                                                    np.asarray,\n","                                                    model.normalize,\n","                                                    imutils.HWC_to_CHW\n","                                                ],\n","                                                label_transform_list=[\n","                                                    None,\n","                                                    None,\n","                                                    None,\n","                                                    imutils.AvgPool2d(8)\n","                                                ])\n","    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n","                                   pin_memory=True, drop_last=True, worker_init_fn=worker_init_fn)\n","    max_step = len(train_dataset) // batch_size * epoch\n","    print('max step: ', max_step)\n","    param_groups = model.get_parameter_groups()\n","\n","    lambda1 = lambda epoch: 0.88#(1 - epoch* (len(train_dataset) // batch_size) / max_step) ** momentum\n","    optimizer = torch.optim.SGD([\n","        {'params': param_groups[0], 'lr': lr, 'weight_decay': weight_decay},\n","        {'params': param_groups[1], 'lr': 2*lr, 'weight_decay': 0},\n","        {'params': param_groups[2], 'lr': 10*lr, 'weight_decay': weight_decay},\n","        {'params': param_groups[3], 'lr': 20*lr, 'weight_decay': 0}\n","        ], lr=lr, weight_decay=weight_decay)\n","    \n","\n","    scheduler = MultiplicativeLR(optimizer, lr_lambda=[lambda1, lambda1, lambda1, lambda1])\n","\n","    PARAMS = {'dataset':dataset,\n","                        'network':network,\n","                        'epoch_nr': epoch,\n","                        'batch_size': batch_size,\n","                        'optimizer': 'SGD'}\n","\n","    neptune.create_experiment(session_name, params=PARAMS)\n","\n","    # Load weights\n","    if resume:\n","      model, optimizer, current_epoch = load_ckp(snapshot_dir+'/voc_epoch_1.pth', model, optimizer)\n","      # state_dict = torch.load(snapshot_dir+'/voc_epoch_1.pth')\n","      # global_counter = 1300 - 1\n","      # optimizer.global_step = global_counter - 1\n","      \n","    else:\n","      if weights[-7:] == '.params':\n","          import network.resnet38d\n","          weights_dict = network.resnet38d.convert_mxnet_to_torch(weights)\n","          model.load_state_dict(weights_dict, strict=False)\n","      else:\n","          weights_dict = torch.load(weights)\n","          model.load_state_dict(weights_dict, strict=False)\n","    \n","    model = torch.nn.DataParallel(model).cuda()\n","    model.to('cuda:0')\n","    # Set project and create run\n","    run = neptune.init(project_qualified_name='ch.kalavritinos/OAA', api_token=NEPTUNE_TOKEN)\n","    # print('Create class ...')\n","    \n","    model.train()\n","\n","    avg_meter = pyutils.AverageMeter('loss', 'bg_loss', 'fg_loss', 'neg_loss', 'bg_cnt', 'fg_cnt', 'neg_cnt')\n","    timer = pyutils.Timer(\"Session started: \")\n","\n","\n","    while current_epoch < epoch:    \n","        for iter, pack in enumerate(train_data_loader):\n","            aff = model.forward(pack[0].to('cuda:0'))\n","            # aff.to('cuda:0')\n","\n","            bg_label = pack[1][0].cuda(non_blocking=True)\n","            fg_label = pack[1][1].cuda(non_blocking=True)\n","            neg_label = pack[1][2].cuda(non_blocking=True)\n","\n","            bg_count = torch.sum(bg_label) + 1e-5\n","            fg_count = torch.sum(fg_label) + 1e-5\n","            neg_count = torch.sum(neg_label) + 1e-5\n","            bg_count.to('cuda:0'), fg_count.to('cuda:0'), neg_count.to('cuda:0')\n","\n","            bg_loss = torch.sum(- bg_label * torch.log(aff + 1e-5)) / bg_count\n","            fg_loss = torch.sum(- fg_label * torch.log(aff + 1e-5)) / fg_count\n","            neg_loss = torch.sum(- neg_label * torch.log(1. + 1e-5 - aff)) / neg_count\n","            bg_loss.to('cuda:0'), fg_loss.to('cuda:0'), neg_loss.to('cuda:0')\n","\n","            loss = bg_loss/4 + fg_loss/4 + neg_loss/2\n","            loss.to('cuda:0')\n","            # print(loss)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            avg_meter.add({\n","                'loss': loss.item(),\n","                'bg_loss': bg_loss.item(), 'fg_loss': fg_loss.item(), 'neg_loss': neg_loss.item(),\n","                'bg_cnt': bg_count.item(), 'fg_cnt': fg_count.item(), 'neg_cnt': neg_count.item()\n","            })\n","\n","            if (global_counter - 1) % disp_interval == 0:\n","                timer.update_progress(global_counter / max_step)\n","\n","                print('Iter:%5d/%5d' % (global_counter-1, max_step),\n","                      'loss:%.4f %.4f %.4f %.4f' % avg_meter.get('loss', 'bg_loss', 'fg_loss', 'neg_loss'),\n","                      'cnt:%.0f %.0f %.0f' % avg_meter.get('bg_cnt', 'fg_cnt', 'neg_cnt'),\n","                      'imps:%.1f' % ((iter+1) * batch_size / timer.get_stage_elapsed()),\n","                      'lr: %.4f' % (optimizer.param_groups[0]['lr']), flush=True)\n","\n","                # neptune.log_metric('loss', avg_meter.get('loss'))\n","                # neptune.log_metric('bg_loss', avg_meter.get('bg_loss'))\n","                # neptune.log_metric('fg_loss', avg_meter.get('fg_loss'))\n","                # neptune.log_metric('neg_loss', avg_meter.get('neg_loss'))\n","                avg_meter.pop()\n","            global_counter = global_counter + 1\n","            \n","\n","        save_checkpoint(\n","                          {'epoch': current_epoch,\n","                            'state_dict': model.state_dict(),\n","                            'optimizer': optimizer.state_dict(),\n","                            'global_counter': global_counter\n","                          }, is_best=False,\n","                          filename='%s_epoch_%d.pth' %(dataset, current_epoch))\n","        # torch.save(model.module.state_dict(), session_name + current_epoch + '.pth')\n","        # print('model saved!...')\n","        current_epoch += 1\n","        scheduler.step()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMOKcM4qPP7d"},"source":["# train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cbZOAhcEz2LU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630922007104,"user_tz":-180,"elapsed":1400,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"421dfaec-f55f-4b05-9861-cfb251d76e65"},"source":["model = getattr(importlib.import_module(\"network.resnet38_aff\"), 'Net')()\n","model.state_dict().keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['conv1a.weight', 'b2.bn_branch2a.weight', 'b2.bn_branch2a.bias', 'b2.bn_branch2a.running_mean', 'b2.bn_branch2a.running_var', 'b2.bn_branch2a.num_batches_tracked', 'b2.conv_branch2a.weight', 'b2.bn_branch2b1.weight', 'b2.bn_branch2b1.bias', 'b2.bn_branch2b1.running_mean', 'b2.bn_branch2b1.running_var', 'b2.bn_branch2b1.num_batches_tracked', 'b2.conv_branch2b1.weight', 'b2.conv_branch1.weight', 'b2_1.bn_branch2a.weight', 'b2_1.bn_branch2a.bias', 'b2_1.bn_branch2a.running_mean', 'b2_1.bn_branch2a.running_var', 'b2_1.bn_branch2a.num_batches_tracked', 'b2_1.conv_branch2a.weight', 'b2_1.bn_branch2b1.weight', 'b2_1.bn_branch2b1.bias', 'b2_1.bn_branch2b1.running_mean', 'b2_1.bn_branch2b1.running_var', 'b2_1.bn_branch2b1.num_batches_tracked', 'b2_1.conv_branch2b1.weight', 'b2_2.bn_branch2a.weight', 'b2_2.bn_branch2a.bias', 'b2_2.bn_branch2a.running_mean', 'b2_2.bn_branch2a.running_var', 'b2_2.bn_branch2a.num_batches_tracked', 'b2_2.conv_branch2a.weight', 'b2_2.bn_branch2b1.weight', 'b2_2.bn_branch2b1.bias', 'b2_2.bn_branch2b1.running_mean', 'b2_2.bn_branch2b1.running_var', 'b2_2.bn_branch2b1.num_batches_tracked', 'b2_2.conv_branch2b1.weight', 'b3.bn_branch2a.weight', 'b3.bn_branch2a.bias', 'b3.bn_branch2a.running_mean', 'b3.bn_branch2a.running_var', 'b3.bn_branch2a.num_batches_tracked', 'b3.conv_branch2a.weight', 'b3.bn_branch2b1.weight', 'b3.bn_branch2b1.bias', 'b3.bn_branch2b1.running_mean', 'b3.bn_branch2b1.running_var', 'b3.bn_branch2b1.num_batches_tracked', 'b3.conv_branch2b1.weight', 'b3.conv_branch1.weight', 'b3_1.bn_branch2a.weight', 'b3_1.bn_branch2a.bias', 'b3_1.bn_branch2a.running_mean', 'b3_1.bn_branch2a.running_var', 'b3_1.bn_branch2a.num_batches_tracked', 'b3_1.conv_branch2a.weight', 'b3_1.bn_branch2b1.weight', 'b3_1.bn_branch2b1.bias', 'b3_1.bn_branch2b1.running_mean', 'b3_1.bn_branch2b1.running_var', 'b3_1.bn_branch2b1.num_batches_tracked', 'b3_1.conv_branch2b1.weight', 'b3_2.bn_branch2a.weight', 'b3_2.bn_branch2a.bias', 'b3_2.bn_branch2a.running_mean', 'b3_2.bn_branch2a.running_var', 'b3_2.bn_branch2a.num_batches_tracked', 'b3_2.conv_branch2a.weight', 'b3_2.bn_branch2b1.weight', 'b3_2.bn_branch2b1.bias', 'b3_2.bn_branch2b1.running_mean', 'b3_2.bn_branch2b1.running_var', 'b3_2.bn_branch2b1.num_batches_tracked', 'b3_2.conv_branch2b1.weight', 'b4.bn_branch2a.weight', 'b4.bn_branch2a.bias', 'b4.bn_branch2a.running_mean', 'b4.bn_branch2a.running_var', 'b4.bn_branch2a.num_batches_tracked', 'b4.conv_branch2a.weight', 'b4.bn_branch2b1.weight', 'b4.bn_branch2b1.bias', 'b4.bn_branch2b1.running_mean', 'b4.bn_branch2b1.running_var', 'b4.bn_branch2b1.num_batches_tracked', 'b4.conv_branch2b1.weight', 'b4.conv_branch1.weight', 'b4_1.bn_branch2a.weight', 'b4_1.bn_branch2a.bias', 'b4_1.bn_branch2a.running_mean', 'b4_1.bn_branch2a.running_var', 'b4_1.bn_branch2a.num_batches_tracked', 'b4_1.conv_branch2a.weight', 'b4_1.bn_branch2b1.weight', 'b4_1.bn_branch2b1.bias', 'b4_1.bn_branch2b1.running_mean', 'b4_1.bn_branch2b1.running_var', 'b4_1.bn_branch2b1.num_batches_tracked', 'b4_1.conv_branch2b1.weight', 'b4_2.bn_branch2a.weight', 'b4_2.bn_branch2a.bias', 'b4_2.bn_branch2a.running_mean', 'b4_2.bn_branch2a.running_var', 'b4_2.bn_branch2a.num_batches_tracked', 'b4_2.conv_branch2a.weight', 'b4_2.bn_branch2b1.weight', 'b4_2.bn_branch2b1.bias', 'b4_2.bn_branch2b1.running_mean', 'b4_2.bn_branch2b1.running_var', 'b4_2.bn_branch2b1.num_batches_tracked', 'b4_2.conv_branch2b1.weight', 'b4_3.bn_branch2a.weight', 'b4_3.bn_branch2a.bias', 'b4_3.bn_branch2a.running_mean', 'b4_3.bn_branch2a.running_var', 'b4_3.bn_branch2a.num_batches_tracked', 'b4_3.conv_branch2a.weight', 'b4_3.bn_branch2b1.weight', 'b4_3.bn_branch2b1.bias', 'b4_3.bn_branch2b1.running_mean', 'b4_3.bn_branch2b1.running_var', 'b4_3.bn_branch2b1.num_batches_tracked', 'b4_3.conv_branch2b1.weight', 'b4_4.bn_branch2a.weight', 'b4_4.bn_branch2a.bias', 'b4_4.bn_branch2a.running_mean', 'b4_4.bn_branch2a.running_var', 'b4_4.bn_branch2a.num_batches_tracked', 'b4_4.conv_branch2a.weight', 'b4_4.bn_branch2b1.weight', 'b4_4.bn_branch2b1.bias', 'b4_4.bn_branch2b1.running_mean', 'b4_4.bn_branch2b1.running_var', 'b4_4.bn_branch2b1.num_batches_tracked', 'b4_4.conv_branch2b1.weight', 'b4_5.bn_branch2a.weight', 'b4_5.bn_branch2a.bias', 'b4_5.bn_branch2a.running_mean', 'b4_5.bn_branch2a.running_var', 'b4_5.bn_branch2a.num_batches_tracked', 'b4_5.conv_branch2a.weight', 'b4_5.bn_branch2b1.weight', 'b4_5.bn_branch2b1.bias', 'b4_5.bn_branch2b1.running_mean', 'b4_5.bn_branch2b1.running_var', 'b4_5.bn_branch2b1.num_batches_tracked', 'b4_5.conv_branch2b1.weight', 'b5.bn_branch2a.weight', 'b5.bn_branch2a.bias', 'b5.bn_branch2a.running_mean', 'b5.bn_branch2a.running_var', 'b5.bn_branch2a.num_batches_tracked', 'b5.conv_branch2a.weight', 'b5.bn_branch2b1.weight', 'b5.bn_branch2b1.bias', 'b5.bn_branch2b1.running_mean', 'b5.bn_branch2b1.running_var', 'b5.bn_branch2b1.num_batches_tracked', 'b5.conv_branch2b1.weight', 'b5.conv_branch1.weight', 'b5_1.bn_branch2a.weight', 'b5_1.bn_branch2a.bias', 'b5_1.bn_branch2a.running_mean', 'b5_1.bn_branch2a.running_var', 'b5_1.bn_branch2a.num_batches_tracked', 'b5_1.conv_branch2a.weight', 'b5_1.bn_branch2b1.weight', 'b5_1.bn_branch2b1.bias', 'b5_1.bn_branch2b1.running_mean', 'b5_1.bn_branch2b1.running_var', 'b5_1.bn_branch2b1.num_batches_tracked', 'b5_1.conv_branch2b1.weight', 'b5_2.bn_branch2a.weight', 'b5_2.bn_branch2a.bias', 'b5_2.bn_branch2a.running_mean', 'b5_2.bn_branch2a.running_var', 'b5_2.bn_branch2a.num_batches_tracked', 'b5_2.conv_branch2a.weight', 'b5_2.bn_branch2b1.weight', 'b5_2.bn_branch2b1.bias', 'b5_2.bn_branch2b1.running_mean', 'b5_2.bn_branch2b1.running_var', 'b5_2.bn_branch2b1.num_batches_tracked', 'b5_2.conv_branch2b1.weight', 'b6.bn_branch2a.weight', 'b6.bn_branch2a.bias', 'b6.bn_branch2a.running_mean', 'b6.bn_branch2a.running_var', 'b6.bn_branch2a.num_batches_tracked', 'b6.conv_branch2a.weight', 'b6.bn_branch2b1.weight', 'b6.bn_branch2b1.bias', 'b6.bn_branch2b1.running_mean', 'b6.bn_branch2b1.running_var', 'b6.bn_branch2b1.num_batches_tracked', 'b6.conv_branch2b1.weight', 'b6.bn_branch2b2.weight', 'b6.bn_branch2b2.bias', 'b6.bn_branch2b2.running_mean', 'b6.bn_branch2b2.running_var', 'b6.bn_branch2b2.num_batches_tracked', 'b6.conv_branch2b2.weight', 'b6.conv_branch1.weight', 'b7.bn_branch2a.weight', 'b7.bn_branch2a.bias', 'b7.bn_branch2a.running_mean', 'b7.bn_branch2a.running_var', 'b7.bn_branch2a.num_batches_tracked', 'b7.conv_branch2a.weight', 'b7.bn_branch2b1.weight', 'b7.bn_branch2b1.bias', 'b7.bn_branch2b1.running_mean', 'b7.bn_branch2b1.running_var', 'b7.bn_branch2b1.num_batches_tracked', 'b7.conv_branch2b1.weight', 'b7.bn_branch2b2.weight', 'b7.bn_branch2b2.bias', 'b7.bn_branch2b2.running_mean', 'b7.bn_branch2b2.running_var', 'b7.bn_branch2b2.num_batches_tracked', 'b7.conv_branch2b2.weight', 'b7.conv_branch1.weight', 'bn7.weight', 'bn7.bias', 'bn7.running_mean', 'bn7.running_var', 'bn7.num_batches_tracked', 'f8_3.weight', 'f8_4.weight', 'f8_5.weight', 'f9.weight'])"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-j6xunj3U7E","executionInfo":{"status":"ok","timestamp":1630922009357,"user_tz":-180,"elapsed":1540,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"53340faf-da98-4c9c-a2ef-4baeca920df7"},"source":["import network.resnet38_aff\n","model = network.resnet38_aff.Net()\n","model.state_dict().keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['conv1a.weight', 'b2.bn_branch2a.weight', 'b2.bn_branch2a.bias', 'b2.bn_branch2a.running_mean', 'b2.bn_branch2a.running_var', 'b2.bn_branch2a.num_batches_tracked', 'b2.conv_branch2a.weight', 'b2.bn_branch2b1.weight', 'b2.bn_branch2b1.bias', 'b2.bn_branch2b1.running_mean', 'b2.bn_branch2b1.running_var', 'b2.bn_branch2b1.num_batches_tracked', 'b2.conv_branch2b1.weight', 'b2.conv_branch1.weight', 'b2_1.bn_branch2a.weight', 'b2_1.bn_branch2a.bias', 'b2_1.bn_branch2a.running_mean', 'b2_1.bn_branch2a.running_var', 'b2_1.bn_branch2a.num_batches_tracked', 'b2_1.conv_branch2a.weight', 'b2_1.bn_branch2b1.weight', 'b2_1.bn_branch2b1.bias', 'b2_1.bn_branch2b1.running_mean', 'b2_1.bn_branch2b1.running_var', 'b2_1.bn_branch2b1.num_batches_tracked', 'b2_1.conv_branch2b1.weight', 'b2_2.bn_branch2a.weight', 'b2_2.bn_branch2a.bias', 'b2_2.bn_branch2a.running_mean', 'b2_2.bn_branch2a.running_var', 'b2_2.bn_branch2a.num_batches_tracked', 'b2_2.conv_branch2a.weight', 'b2_2.bn_branch2b1.weight', 'b2_2.bn_branch2b1.bias', 'b2_2.bn_branch2b1.running_mean', 'b2_2.bn_branch2b1.running_var', 'b2_2.bn_branch2b1.num_batches_tracked', 'b2_2.conv_branch2b1.weight', 'b3.bn_branch2a.weight', 'b3.bn_branch2a.bias', 'b3.bn_branch2a.running_mean', 'b3.bn_branch2a.running_var', 'b3.bn_branch2a.num_batches_tracked', 'b3.conv_branch2a.weight', 'b3.bn_branch2b1.weight', 'b3.bn_branch2b1.bias', 'b3.bn_branch2b1.running_mean', 'b3.bn_branch2b1.running_var', 'b3.bn_branch2b1.num_batches_tracked', 'b3.conv_branch2b1.weight', 'b3.conv_branch1.weight', 'b3_1.bn_branch2a.weight', 'b3_1.bn_branch2a.bias', 'b3_1.bn_branch2a.running_mean', 'b3_1.bn_branch2a.running_var', 'b3_1.bn_branch2a.num_batches_tracked', 'b3_1.conv_branch2a.weight', 'b3_1.bn_branch2b1.weight', 'b3_1.bn_branch2b1.bias', 'b3_1.bn_branch2b1.running_mean', 'b3_1.bn_branch2b1.running_var', 'b3_1.bn_branch2b1.num_batches_tracked', 'b3_1.conv_branch2b1.weight', 'b3_2.bn_branch2a.weight', 'b3_2.bn_branch2a.bias', 'b3_2.bn_branch2a.running_mean', 'b3_2.bn_branch2a.running_var', 'b3_2.bn_branch2a.num_batches_tracked', 'b3_2.conv_branch2a.weight', 'b3_2.bn_branch2b1.weight', 'b3_2.bn_branch2b1.bias', 'b3_2.bn_branch2b1.running_mean', 'b3_2.bn_branch2b1.running_var', 'b3_2.bn_branch2b1.num_batches_tracked', 'b3_2.conv_branch2b1.weight', 'b4.bn_branch2a.weight', 'b4.bn_branch2a.bias', 'b4.bn_branch2a.running_mean', 'b4.bn_branch2a.running_var', 'b4.bn_branch2a.num_batches_tracked', 'b4.conv_branch2a.weight', 'b4.bn_branch2b1.weight', 'b4.bn_branch2b1.bias', 'b4.bn_branch2b1.running_mean', 'b4.bn_branch2b1.running_var', 'b4.bn_branch2b1.num_batches_tracked', 'b4.conv_branch2b1.weight', 'b4.conv_branch1.weight', 'b4_1.bn_branch2a.weight', 'b4_1.bn_branch2a.bias', 'b4_1.bn_branch2a.running_mean', 'b4_1.bn_branch2a.running_var', 'b4_1.bn_branch2a.num_batches_tracked', 'b4_1.conv_branch2a.weight', 'b4_1.bn_branch2b1.weight', 'b4_1.bn_branch2b1.bias', 'b4_1.bn_branch2b1.running_mean', 'b4_1.bn_branch2b1.running_var', 'b4_1.bn_branch2b1.num_batches_tracked', 'b4_1.conv_branch2b1.weight', 'b4_2.bn_branch2a.weight', 'b4_2.bn_branch2a.bias', 'b4_2.bn_branch2a.running_mean', 'b4_2.bn_branch2a.running_var', 'b4_2.bn_branch2a.num_batches_tracked', 'b4_2.conv_branch2a.weight', 'b4_2.bn_branch2b1.weight', 'b4_2.bn_branch2b1.bias', 'b4_2.bn_branch2b1.running_mean', 'b4_2.bn_branch2b1.running_var', 'b4_2.bn_branch2b1.num_batches_tracked', 'b4_2.conv_branch2b1.weight', 'b4_3.bn_branch2a.weight', 'b4_3.bn_branch2a.bias', 'b4_3.bn_branch2a.running_mean', 'b4_3.bn_branch2a.running_var', 'b4_3.bn_branch2a.num_batches_tracked', 'b4_3.conv_branch2a.weight', 'b4_3.bn_branch2b1.weight', 'b4_3.bn_branch2b1.bias', 'b4_3.bn_branch2b1.running_mean', 'b4_3.bn_branch2b1.running_var', 'b4_3.bn_branch2b1.num_batches_tracked', 'b4_3.conv_branch2b1.weight', 'b4_4.bn_branch2a.weight', 'b4_4.bn_branch2a.bias', 'b4_4.bn_branch2a.running_mean', 'b4_4.bn_branch2a.running_var', 'b4_4.bn_branch2a.num_batches_tracked', 'b4_4.conv_branch2a.weight', 'b4_4.bn_branch2b1.weight', 'b4_4.bn_branch2b1.bias', 'b4_4.bn_branch2b1.running_mean', 'b4_4.bn_branch2b1.running_var', 'b4_4.bn_branch2b1.num_batches_tracked', 'b4_4.conv_branch2b1.weight', 'b4_5.bn_branch2a.weight', 'b4_5.bn_branch2a.bias', 'b4_5.bn_branch2a.running_mean', 'b4_5.bn_branch2a.running_var', 'b4_5.bn_branch2a.num_batches_tracked', 'b4_5.conv_branch2a.weight', 'b4_5.bn_branch2b1.weight', 'b4_5.bn_branch2b1.bias', 'b4_5.bn_branch2b1.running_mean', 'b4_5.bn_branch2b1.running_var', 'b4_5.bn_branch2b1.num_batches_tracked', 'b4_5.conv_branch2b1.weight', 'b5.bn_branch2a.weight', 'b5.bn_branch2a.bias', 'b5.bn_branch2a.running_mean', 'b5.bn_branch2a.running_var', 'b5.bn_branch2a.num_batches_tracked', 'b5.conv_branch2a.weight', 'b5.bn_branch2b1.weight', 'b5.bn_branch2b1.bias', 'b5.bn_branch2b1.running_mean', 'b5.bn_branch2b1.running_var', 'b5.bn_branch2b1.num_batches_tracked', 'b5.conv_branch2b1.weight', 'b5.conv_branch1.weight', 'b5_1.bn_branch2a.weight', 'b5_1.bn_branch2a.bias', 'b5_1.bn_branch2a.running_mean', 'b5_1.bn_branch2a.running_var', 'b5_1.bn_branch2a.num_batches_tracked', 'b5_1.conv_branch2a.weight', 'b5_1.bn_branch2b1.weight', 'b5_1.bn_branch2b1.bias', 'b5_1.bn_branch2b1.running_mean', 'b5_1.bn_branch2b1.running_var', 'b5_1.bn_branch2b1.num_batches_tracked', 'b5_1.conv_branch2b1.weight', 'b5_2.bn_branch2a.weight', 'b5_2.bn_branch2a.bias', 'b5_2.bn_branch2a.running_mean', 'b5_2.bn_branch2a.running_var', 'b5_2.bn_branch2a.num_batches_tracked', 'b5_2.conv_branch2a.weight', 'b5_2.bn_branch2b1.weight', 'b5_2.bn_branch2b1.bias', 'b5_2.bn_branch2b1.running_mean', 'b5_2.bn_branch2b1.running_var', 'b5_2.bn_branch2b1.num_batches_tracked', 'b5_2.conv_branch2b1.weight', 'b6.bn_branch2a.weight', 'b6.bn_branch2a.bias', 'b6.bn_branch2a.running_mean', 'b6.bn_branch2a.running_var', 'b6.bn_branch2a.num_batches_tracked', 'b6.conv_branch2a.weight', 'b6.bn_branch2b1.weight', 'b6.bn_branch2b1.bias', 'b6.bn_branch2b1.running_mean', 'b6.bn_branch2b1.running_var', 'b6.bn_branch2b1.num_batches_tracked', 'b6.conv_branch2b1.weight', 'b6.bn_branch2b2.weight', 'b6.bn_branch2b2.bias', 'b6.bn_branch2b2.running_mean', 'b6.bn_branch2b2.running_var', 'b6.bn_branch2b2.num_batches_tracked', 'b6.conv_branch2b2.weight', 'b6.conv_branch1.weight', 'b7.bn_branch2a.weight', 'b7.bn_branch2a.bias', 'b7.bn_branch2a.running_mean', 'b7.bn_branch2a.running_var', 'b7.bn_branch2a.num_batches_tracked', 'b7.conv_branch2a.weight', 'b7.bn_branch2b1.weight', 'b7.bn_branch2b1.bias', 'b7.bn_branch2b1.running_mean', 'b7.bn_branch2b1.running_var', 'b7.bn_branch2b1.num_batches_tracked', 'b7.conv_branch2b1.weight', 'b7.bn_branch2b2.weight', 'b7.bn_branch2b2.bias', 'b7.bn_branch2b2.running_mean', 'b7.bn_branch2b2.running_var', 'b7.bn_branch2b2.num_batches_tracked', 'b7.conv_branch2b2.weight', 'b7.conv_branch1.weight', 'bn7.weight', 'bn7.bias', 'bn7.running_mean', 'bn7.running_var', 'bn7.num_batches_tracked', 'f8_3.weight', 'f8_4.weight', 'f8_5.weight', 'f9.weight'])"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"0R-_hGsk5afs"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","  state_dict = torch.load(snapshot_dir+'/voc_epoch_7.pth')\n","else:\n","  state_dict = torch.load(snapshot_dir+'/voc_epoch_7.pth', map_location=torch.device('cpu'))\n","state_dict['optimizer']['state'].keys()\n","# new_state_dict.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sH3JGSFW_bIb"},"source":["param_groups = model.get_parameter_groups()\n","optimizer = torchutils.PolyOptimizer([\n","        {'params': param_groups[0], 'lr': lr, 'weight_decay': weight_decay},\n","        {'params': param_groups[1], 'lr': 2*lr, 'weight_decay': 0},\n","        {'params': param_groups[2], 'lr': 10*lr, 'weight_decay': weight_decay},\n","        {'params': param_groups[3], 'lr': 20*lr, 'weight_decay': 0}\n","        ], lr=lr, weight_decay=weight_decay, max_step=9915)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdmkPcQe1kWE"},"source":["from collections import OrderedDict\n","\n","new_state_dict = OrderedDict()\n","for k, v in state_dict['state_dict'].items():\n","    name = k[7:] # remove `module.`\n","    new_state_dict[name] = v\n","# model.load_state_dict(new_state_dict)\n","# print('weights loaded!', current_epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NtwZP8uTGxkl"},"source":["from torch.utils.data import Dataset\n","\n","IMG_FOLDER_NAME = \"JPEGImages\"\n","ANNOT_FOLDER_NAME = \"Annotations\"\n","\n","CAT_LIST = ['aeroplane', 'bicycle', 'bird', 'boat',\n","        'bottle', 'bus', 'car', 'cat', 'chair',\n","        'cow', 'diningtable', 'dog', 'horse',\n","        'motorbike', 'person', 'pottedplant',\n","        'sheep', 'sofa', 'train',\n","        'tvmonitor']\n","\n","CAT_NAME_TO_NUM = dict(zip(CAT_LIST,range(len(CAT_LIST))))\n","\n","class VOC12ImageDataset(Dataset):\n","    def __init__(self, img_name_list_path, voc12_root, transform=None):\n","        # print('init of voc12 image dataset   ...   \\n')\n","        self.img_name_list = load_img_name_list(img_name_list_path)\n","        # print('image name list len:', len(self.img_name_list))\n","        self.voc12_root = voc12_root\n","        self.transform = transform\n","        # print('------------')\n","\n","    def __len__(self):\n","        # print('voc12 image dataset length 88888888')\n","        return len(self.img_name_list)\n","\n","    def __getitem__(self, idx):\n","        # print('start of get item voc12 image dataset')\n","        name = self.img_name_list[idx]\n","        img = PIL.Image.open(get_img_path(name, self.voc12_root)).convert(\"RGB\")\n","        # print('image opened!')\n","\n","        if self.transform:\n","            img = self.transform(img)\n","        return name, img\n","\n","\n","\n","\n","class ExtractAffinityLabelInRadius():\n","    def __init__(self, cropsize, radius=5):\n","        # print('init of extract label')\n","        self.radius = radius\n","        self.search_dist = []\n","\n","        for x in range(1, radius):\n","            self.search_dist.append((0, x))\n","\n","        for y in range(1, radius):\n","            for x in range(-radius+1, radius):\n","                if x*x + y*y < radius*radius:\n","                    self.search_dist.append((y, x))\n","\n","        self.radius_floor = radius-1\n","\n","        self.crop_height = cropsize - self.radius_floor\n","        self.crop_width = cropsize - 2 * self.radius_floor\n","        # print('end of extract label')\n","        return\n","\n","    def __call__(self, label):\n","        # print('start of _call_ in extract affinity ')\n","        labels_from = label[:-self.radius_floor, self.radius_floor:-self.radius_floor]\n","        labels_from = np.reshape(labels_from, [-1])\n","\n","        labels_to_list = []\n","        valid_pair_list = []\n","\n","        for dy, dx in self.search_dist:\n","            labels_to = label[dy:dy+self.crop_height, self.radius_floor+dx:self.radius_floor+dx+self.crop_width]\n","            labels_to = np.reshape(labels_to, [-1])\n","\n","            valid_pair = np.logical_and(np.less(labels_to, 255), np.less(labels_from, 255))\n","\n","            labels_to_list.append(labels_to)\n","            valid_pair_list.append(valid_pair)\n","\n","        bc_labels_from = np.expand_dims(labels_from, 0)\n","        concat_labels_to = np.stack(labels_to_list)\n","        concat_valid_pair = np.stack(valid_pair_list)\n","\n","        pos_affinity_label = np.equal(bc_labels_from, concat_labels_to)\n","\n","        bg_pos_affinity_label = np.logical_and(pos_affinity_label, np.equal(bc_labels_from, 0)).astype(np.float32)\n","\n","        fg_pos_affinity_label = np.logical_and(np.logical_and(pos_affinity_label, np.not_equal(bc_labels_from, 0)), concat_valid_pair).astype(np.float32)\n","\n","        neg_affinity_label = np.logical_and(np.logical_not(pos_affinity_label), concat_valid_pair).astype(np.float32)\n","        # print('affinity labels before return extract affinity')\n","        return torch.from_numpy(bg_pos_affinity_label), torch.from_numpy(fg_pos_affinity_label), torch.from_numpy(neg_affinity_label)\n","\n","\n","\n","def load_image_label_from_xml(img_name, voc12_root):\n","    from xml.dom import minidom             # to parse and modify xml docs\n","\n","    el_list = minidom.parse(os.path.join(voc12_root, ANNOT_FOLDER_NAME,img_name + '.xml')).getElementsByTagName('name')\n","\n","    multi_cls_lab = np.zeros((20), np.float32)      # init\n","\n","    for el in el_list:\n","        cat_name = el.firstChild.data\n","        if cat_name in CAT_LIST:\n","            cat_num = CAT_NAME_TO_NUM[cat_name]\n","            multi_cls_lab[cat_num] = 1.0\n","\n","    return multi_cls_lab            # return array with classes (has 0 or 1)\n","\n","def load_image_label_list_from_xml(img_name_list, voc12_root):\n","    return [load_image_label_from_xml(img_name, voc12_root) for img_name in img_name_list]\n","\n","def load_image_label_list_from_npy(img_name_list):\n","    # print('load image label list from npy')\n","    cls_labels_dict = np.load('voc12/cls_labels.npy', allow_pickle=True).item()\n","\n","    return [cls_labels_dict[img_name] for img_name in img_name_list]\n","\n","def get_img_path(img_name, voc12_root):\n","    # print('get img path')\n","    return os.path.join(voc12_root, IMG_FOLDER_NAME, img_name + '.jpg')\n","\n","\n","\n","def load_img_name_list(dataset_path):\n","    img_gt_name_list = open(dataset_path).read().splitlines()\n","    img_name_list = [img_gt_name.split(' ')[0][-15:-4] for img_gt_name in img_gt_name_list]\n","    return img_name_list\n","\n","\n","\n","\n","\n","class VOC12AffDataset(VOC12ImageDataset):\n","    def __init__(self, img_name_list_path, label_la_dir, label_ha_dir, cropsize, voc12_root, radius=5,\n","                 joint_transform_list=None, img_transform_list=None, label_transform_list=None):\n","        # print('voc12 Aff dataset init ... \\n')\n","        super().__init__(img_name_list_path, voc12_root, transform=None)\n","\n","        self.label_la_dir = label_la_dir\n","        self.label_ha_dir = label_ha_dir\n","        self.voc12_root = voc12_root\n","\n","        self.joint_transform_list = joint_transform_list\n","        self.img_transform_list = img_transform_list\n","        self.label_transform_list = label_transform_list\n","        # print('before ExtractAffinityLabelInRadius')\n","        self.extract_aff_lab_func = ExtractAffinityLabelInRadius(cropsize=cropsize//8, radius=radius)\n","        # print('After ...')\n","\n","    def __len__(self):\n","        # print('voc12 aff class _len_ : ', len(self.img_name_list))\n","        return len(self.img_name_list)\n","\n","    def __getitem__(self, idx):\n","        name, img = super().__getitem__(idx)\n","        label_la_path = os.path.join(self.label_la_dir, name + '.npy')\n","        label_ha_path = os.path.join(self.label_ha_dir, name + '.npy')\n","        label_la = np.load(label_la_path, allow_pickle=True).item()\n","        label_ha = np.load(label_ha_path, allow_pickle=True).item()\n","\n","        label = np.array(list(label_la.values()) + list(label_ha.values()))\n","        label = np.transpose(label, (1, 2, 0))\n","        # print('before joint TF')\n","        for joint_transform, img_transform, label_transform \\\n","                in zip(self.joint_transform_list, self.img_transform_list, self.label_transform_list):\n","\n","            if joint_transform:\n","                img_label = np.concatenate((img, label), axis=-1)\n","                img_label = joint_transform(img_label)\n","                img = img_label[..., :3]\n","                label = img_label[..., 3:]\n","\n","            if img_transform:\n","                img = img_transform(img)\n","            if label_transform:\n","                label = label_transform(label)\n","\n","        no_score_region = np.max(label, -1) < 1e-5\n","        label_la, label_ha = np.array_split(label, 2, axis=-1)\n","        label_la = np.argmax(label_la, axis=-1).astype(np.uint8)\n","        label_ha = np.argmax(label_ha, axis=-1).astype(np.uint8)\n","        label = label_la.copy()\n","        label[label_la == 0] = 255\n","        label[label_ha == 0] = 0\n","        label[no_score_region] = 255 # mostly outer of cropped region\n","        # print(type(label), label.shape, label.max(), label.min())     # 28x28 ndarray\n","        label = self.extract_aff_lab_func(label)\n","        # print(type(label))                  # tuple\n","        return img, label\n","\n","\n","        # label : 0=foreground, 1= bg, 2=negative\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMda2E8tmeu_"},"source":["# Kvasir dataloaders\n"]},{"cell_type":"code","metadata":{"id":"Mj6t00Chq1gx"},"source":["class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n","    def __getitem__(self, index):\n","        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n","        path = self.imgs[index][0]\n","        tuple_with_path = (original_tuple + (path,))\n","        return tuple_with_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7T0wBhycSY6L"},"source":["test_dir = os.path.join('/content/drive/My Drive/MAI/thesis/source/kvasir-dataset-v2-folds/1/test')\n","train_dir = os.path.join('/content/drive/My Drive/MAI/thesis/source/kvasir-dataset-v2-folds/1/train')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYR_VgRX34Fg"},"source":["def get_data_loader(data_dir, batch_size=32, train=True):\n","    # define how we augment the data for composing the batch-dataset in train and test step\n","    transform = {\n","        'train': transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.CenterCrop(256),\n","            transforms.Resize(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.RandomRotation(90),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","        ]),\n","        'test': transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.CenterCrop(256),\n","            transforms.Resize(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","        ]),\n","    }\n","\n","    # ImageFolder with root directory and defined transformation methods for batch as well as data augmentation\n","    if train:\n","      data = ImageFolderWithPaths(root=data_dir, transform=transform['train'])\n","    else:\n","      data = ImageFolderWithPaths(root=data_dir, transform=transform['test'])\n","    data_loader = torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","    return data.class_to_idx, data_loader "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"11QhWnq6i3mW"},"source":["torch.multiprocessing.freeze_support()\n","mapping, train_data_loader = get_data_loader(data_dir=train_dir, batch_size=32, train=True)\n","mapping, test_data_loader = get_data_loader(data_dir=test_dir, batch_size=32, train=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkMpaZRvEapU"},"source":["import pathlib\n","train_data_dir = pathlib.Path(train_dir)\n","test_data_dir = pathlib.Path(test_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRKCoChSEapW","executionInfo":{"status":"ok","timestamp":1630920177005,"user_tz":-180,"elapsed":356,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"99b8eef6-d408-42fe-ac9c-6e6c64735f73"},"source":["total_train = len(list(train_data_dir.glob('*/*.jpg')))\n","total_val = len(list(test_data_dir.glob('*/*.jpg')))\n","total_train, total_val"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7200, 800)"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U84qMqgHEapY","executionInfo":{"status":"ok","timestamp":1630920177982,"user_tz":-180,"elapsed":211,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"a09c6272-68d5-46df-d3d0-00b7062d9d3b"},"source":["#get the class names\n","CLASS_NAMES = np.array([item.name for item in train_data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n","CLASS_NAMES"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['normal-pylorus', 'polyps', 'normal-z-line', 'normal-cecum',\n","       'esophagitis', 'dyed-lifted-polyps', 'dyed-resection-margins',\n","       'ulcerative-colitis'], dtype='<U22')"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"CSd42Ouekf5n"},"source":["# Define parameters for training\n","dataset = 'kvasirv2'\n","batch_size = 32\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","STEPS_PER_EPOCH = np.ceil(total_train/batch_size)\n","num_classes = len(CLASS_NAMES) # 8\n","\n","epochs = 16\n","lr = 0.001\n","disp_interval = 100\n","weight_decay = 0.0005\n","decay_points = '5, 10'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5WCPavNDoRk","executionInfo":{"status":"ok","timestamp":1630922034740,"user_tz":-180,"elapsed":1898,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"ba78abb3-353c-42af-b864-51442d4ab75e"},"source":["# from utils.LoadData import train_data_loader\n","\n","def worker_init_fn(worker_id):\n","        np.random.seed(1 + worker_id)\n","\n","input_size = 224\n","disp_interval = 50\n","num_classes = 20\n","num_workers = 2\n","threshold = 0.6\n","resume  = True\n","current_epoch = 0\n","batch_size = 16\n","global_counter = 0\n","network=\"network.resnet38_aff\"\n","  \n","if not os.path.exists(snapshot_dir):\n","    os.makedirs(snapshot_dir)\n","\n","model = getattr(importlib.import_module(network), 'Net')()\n","current_epoch = current_epoch\n","train_dataset = VOC12AffDataset(train_list, label_la_dir=la_crf_dir, label_ha_dir=ha_crf_dir,\n","                                            voc12_root=data_root, cropsize=crop_size, radius=5,\n","                                            joint_transform_list=[\n","                                                None,\n","                                                None,\n","                                                imutils.RandomCrop(crop_size),\n","                                                imutils.RandomHorizontalFlip()\n","                                            ],\n","                                            img_transform_list=[\n","                                                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n","                                                np.asarray,\n","                                                model.normalize,\n","                                                imutils.HWC_to_CHW\n","                                            ],\n","                                            label_transform_list=[\n","                                                None,\n","                                                None,\n","                                                None,\n","                                                imutils.AvgPool2d(8)\n","                                            ])\n","train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n","                                pin_memory=True, drop_last=True, worker_init_fn=worker_init_fn)\n","max_step = len(train_dataset) // batch_size * epoch\n","print('max step: ', max_step)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["max step:  5288\n"]}]},{"cell_type":"code","metadata":{"id":"f6ob1vWlX8wN"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision \n","\n","# functions to show an image\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","# get some random training images\n","dataiter = iter(train_data_loader)\n","images, labels, _ = dataiter.next()\n","imshow(torchvision.utils.make_grid(images))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I2Epx3nITF9a","executionInfo":{"status":"ok","timestamp":1630922181737,"user_tz":-180,"elapsed":226,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"53db4075-c8ec-4a87-8b80-97b20e2d7e80"},"source":["device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"mC0cIq3EDsL0"},"source":["model.eval()\n","model.to(device)\n","for iter, pack in enumerate(train_data_loader):\n","      pack[0] = pack[0].to(device)\n","      pack[1][0] = pack[1][0].to(device)        # batch x 34 x 480\n","      pack[1][1] = pack[1][1].to(device)\n","      pack[1][2] = pack[1][2].to(device)\n","\n","      aff = model.forward(pack[0].to(device))\n","      aff.to(device)\n","\n","      bg_label = pack[1][0].cuda(non_blocking=True)\n","      fg_label = pack[1][1].cuda(non_blocking=True)\n","      neg_label = pack[1][2].cuda(non_blocking=True)\n","\n","      bg_count = torch.sum(bg_label) + 1e-5\n","      fg_count = torch.sum(fg_label) + 1e-5\n","      neg_count = torch.sum(neg_label) + 1e-5\n","      bg_count.to(device), fg_count.to(device), neg_count.to(device)\n","\n","      bg_loss = torch.sum(- bg_label * torch.log(aff + 1e-5)) / bg_count\n","      fg_loss = torch.sum(- fg_label * torch.log(aff + 1e-5)) / fg_count\n","      neg_loss = torch.sum(- neg_label * torch.log(1. + 1e-5 - aff)) / neg_count\n","      bg_loss.to(device), fg_loss.to(device), neg_loss.to(device)\n","\n","      loss = bg_loss/4 + fg_loss/4 + neg_loss/2\n","      loss = loss.to(device)\n","      # print(loss)\n","      break\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qH6XgLd6W99E","executionInfo":{"status":"ok","timestamp":1626104170085,"user_tz":-180,"elapsed":302,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"22c8cb12-d404-4be8-f6e2-0e2b83de2760"},"source":["bg_label.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([16, 34, 480])"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4mK4aZ5_N2_Z","executionInfo":{"status":"ok","timestamp":1626104298912,"user_tz":-180,"elapsed":370,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"16d697c5-f4cf-467d-e106-c7f6775d7d32"},"source":["bg_count, fg_count, neg_count"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(113536., device='cuda:0'),\n"," tensor(34238., device='cuda:0'),\n"," tensor(5075., device='cuda:0'))"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"ZPOCbH1VYhXe"},"source":["img = pack[0].to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtPTukFyKTnd"},"source":["import matplotlib.pyplot as plt\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92pRsW1BYi_M","executionInfo":{"status":"ok","timestamp":1626105284980,"user_tz":-180,"elapsed":274,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"71b08bdc-6d61-4edb-c8e8-2a7373840ec2"},"source":["img[5].max(), img[5].min()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(2.6400, device='cuda:0'), tensor(-2.1179, device='cuda:0'))"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"TbzrH9IlbuZy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626105377169,"user_tz":-180,"elapsed":298,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"25452791-c574-4a4a-c4fc-224ade50f373"},"source":["model.normalize"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<network.resnet38d.Normalize at 0x7f5c4b283ad0>"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcYPBLjzXqkX","executionInfo":{"status":"ok","timestamp":1626104508725,"user_tz":-180,"elapsed":331,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"8342de97-ed51-472f-f85d-e3fef4382f35"},"source":["for i in range(0,15):\n","  print(aff[i].max(), aff[i].min())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(0.9990, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8896, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9955, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8800, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9991, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.9122, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9986, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.9008, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9987, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.9226, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9913, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8689, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9940, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8855, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9993, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8730, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9975, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8691, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9982, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8697, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9961, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8699, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9988, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8685, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9916, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.8688, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9931, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.9186, device='cuda:0', grad_fn=<MinBackward1>)\n","tensor(0.9956, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.9238, device='cuda:0', grad_fn=<MinBackward1>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"44ycm8LCGB0f"},"source":["imgs = dat[0]\n","labels = dat[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfgHroo2D3nW","executionInfo":{"status":"ok","timestamp":1626100723245,"user_tz":-180,"elapsed":309,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"4d300d03-6b57-4fad-e611-a6dba8f30c77"},"source":["labels[0].shape, labels[1].shape, labels[2].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([16, 34, 480]),\n"," torch.Size([16, 34, 480]),\n"," torch.Size([16, 34, 480]))"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2OeC9v86GXrh","executionInfo":{"status":"ok","timestamp":1626100873635,"user_tz":-180,"elapsed":373,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"952c427a-a17d-41a9-8dc1-f5337096679f"},"source":["labels[0][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n","        [0., 0., 0.,  ..., 1., 1., 1.],\n","        [0., 0., 0.,  ..., 1., 1., 1.],\n","        ...,\n","        [0., 0., 0.,  ..., 1., 1., 1.],\n","        [0., 0., 0.,  ..., 1., 1., 1.],\n","        [0., 0., 0.,  ..., 1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"Uuz-s3qnKyZ1"},"source":["param_groups = model.get_parameter_groups()\n","optimizer = torchutils.PolyOptimizer([\n","    {'params': param_groups[0], 'lr': lr, 'weight_decay': weight_decay},\n","    {'params': param_groups[1], 'lr': 2*lr, 'weight_decay': 0},\n","    {'params': param_groups[2], 'lr': 10*lr, 'weight_decay': weight_decay},\n","    {'params': param_groups[3], 'lr': 20*lr, 'weight_decay': 0}\n","    ], lr=lr, weight_decay=weight_decay, max_step=max_step)\n","\n","PARAMS = {'dataset':dataset,\n","                    'network':network,\n","                    'epoch_nr': epoch,\n","                    'batch_size': batch_size,\n","                    'optimizer': 'PolyOptimizer'}\n","\n","neptune.create_experiment(session_name, params=PARAMS)\n","\n","# Load weights\n","if resume:\n","  model, optimizer, current_epoch = load_ckp(snapshot_dir+'/voc_epoch_7.pth', model, optimizer)\n","  # state_dict = torch.load(snapshot_dir+'/voc_epoch_1.pth')\n","  optimizer.global_step = global_counter\n","  \n","else:\n","  if weights[-7:] == '.params':\n","      import network.resnet38d\n","      weights_dict = network.resnet38d.convert_mxnet_to_torch(weights)\n","      model.load_state_dict(weights_dict, strict=False)\n","  else:\n","      weights_dict = torch.load(weights)\n","      model.load_state_dict(weights_dict, strict=False)\n","\n","# model = torch.nn.DataParallel(model).cuda()\n","# model.cuda()\n","# Set project and create run\n","run = neptune.init(project_qualified_name='ch.kalavritinos/OAA', api_token=NEPTUNE_TOKEN)\n","# print('Create class ...')\n","model.train()\n","\n","avg_meter = pyutils.AverageMeter('loss', 'bg_loss', 'fg_loss', 'neg_loss', 'bg_cnt', 'fg_cnt', 'neg_cnt')\n","timer = pyutils.Timer(\"Session started: \")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"nEkjFSr28LiI","executionInfo":{"elapsed":15739,"status":"error","timestamp":1622917820754,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"},"user_tz":-120},"outputId":"a1051cf2-0e64-40e1-9bec-c8ea1bff7706"},"source":["while current_epoch < epoch:    \n","    for iter, pack in enumerate(train_data_loader):\n","        pack[0] = pack[0].to('cuda:0')\n","        pack[1][0] = pack[1][0].to('cuda:0')\n","        pack[1][1] = pack[1][1].to('cuda:0')\n","        pack[1][2] = pack[1][2].to('cuda:0')\n","\n","\n","        aff = model.forward(pack[0].to('cuda:0'))\n","        # aff.to('cuda:0')\n","\n","        bg_label = pack[1][0].cuda(non_blocking=True)\n","        fg_label = pack[1][1].cuda(non_blocking=True)\n","        neg_label = pack[1][2].cuda(non_blocking=True)\n","\n","        bg_count = torch.sum(bg_label) + 1e-5\n","        fg_count = torch.sum(fg_label) + 1e-5\n","        neg_count = torch.sum(neg_label) + 1e-5\n","        bg_count.to('cuda:0'), fg_count.to('cuda:0'), neg_count.to('cuda:0')\n","\n","        bg_loss = torch.sum(- bg_label * torch.log(aff + 1e-5)) / bg_count\n","        fg_loss = torch.sum(- fg_label * torch.log(aff + 1e-5)) / fg_count\n","        neg_loss = torch.sum(- neg_label * torch.log(1. + 1e-5 - aff)) / neg_count\n","        bg_loss.to('cuda:0'), fg_loss.to('cuda:0'), neg_loss.to('cuda:0')\n","\n","        loss = bg_loss/4 + fg_loss/4 + neg_loss/2\n","        loss = loss.to('cuda:0')\n","        # print(loss)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        avg_meter.add({\n","            'loss': loss.item(),\n","            'bg_loss': bg_loss.item(), 'fg_loss': fg_loss.item(), 'neg_loss': neg_loss.item(),\n","            'bg_cnt': bg_count.item(), 'fg_cnt': fg_count.item(), 'neg_cnt': neg_count.item()\n","        })\n","\n","        if (global_counter - 1) % disp_interval == 0:\n","            timer.update_progress(global_counter / max_step)\n","\n","            print('Iter:%5d/%5d' % (global_counter-1, max_step),\n","                  'loss:%.4f %.4f %.4f %.4f' % avg_meter.get('loss', 'bg_loss', 'fg_loss', 'neg_loss'),\n","                  'cnt:%.0f %.0f %.0f' % avg_meter.get('bg_cnt', 'fg_cnt', 'neg_cnt'),\n","                  'imps:%.1f' % ((iter+1) * batch_size / timer.get_stage_elapsed()),\n","                  'lr: %.4f' % (optimizer.param_groups[0]['lr']), flush=True)\n","\n","            neptune.log_metric('loss', avg_meter.get('loss'))\n","            neptune.log_metric('bg_loss', avg_meter.get('bg_loss'))\n","            neptune.log_metric('fg_loss', avg_meter.get('fg_loss'))\n","            neptune.log_metric('neg_loss', avg_meter.get('neg_loss'))\n","            avg_meter.pop()\n","        global_counter = global_counter + 1\n","\n","    save_checkpoint(\n","                      {'epoch': current_epoch,\n","                        'state_dict': model.state_dict(),\n","                        'optimizer': optimizer.state_dict(),\n","                        'global_counter': global_counter\n","                      }, is_best=False,\n","                      filename='%s_epoch_%d.pth' %(dataset, current_epoch))\n","    # torch.save(model.module.state_dict(), session_name + current_epoch + '.pth')\n","    # print('model saved!...')\n","    current_epoch += 1\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["max step:  5288\n","https://app.neptune.ai/ch.kalavritinos/OAA/e/OAA-110\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-992a552376c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Load weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ckp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnapshot_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/voc_epoch_1.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0;31m# state_dict = torch.load(snapshot_dir+'/voc_epoch_1.pth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0mglobal_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1300\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-f9a40fa2fbfb>\u001b[0m in \u001b[0;36mload_ckp\u001b[0;34m(checkpoint_fpath, model, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_ckp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_fpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnew_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zATl7JM2LtRi","executionInfo":{"elapsed":243,"status":"ok","timestamp":1622460578113,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"},"user_tz":-120},"outputId":"ddb1f6b2-72f6-4eec-ff2d-e9d0d1c8919e"},"source":["print(bg_loss.device, loss.device, fg_loss.device, neg_loss.device)\n","print(bg_count.device, loss.device, fg_count.device, neg_count.device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0 cuda:0 cuda:0 cuda:0\n","cuda:0 cuda:0 cuda:0 cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oluYnofMBHL","executionInfo":{"elapsed":34,"status":"ok","timestamp":1622460900034,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"},"user_tz":-120},"outputId":"eac5e305-e6c4-485a-ad4f-b569cd90fa75"},"source":["pack[0].device, pack[1][0].device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cuda', index=0), device(type='cuda', index=0))"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gIx2A76HM_a4","executionInfo":{"elapsed":681,"status":"ok","timestamp":1622461001875,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"},"user_tz":-120},"outputId":"ac108b75-0cdb-4210-a98f-ca2199aa05b4"},"source":["pack[0].shape, pack[1][0].shape, pack[1][1].shape, pack[1][2].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([16, 3, 224, 224]),\n"," torch.Size([16, 34, 480]),\n"," torch.Size([16, 34, 480]),\n"," torch.Size([16, 34, 480]))"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yoXGknKNouB","executionInfo":{"elapsed":229,"status":"ok","timestamp":1622461135680,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"},"user_tz":-120},"outputId":"cda48568-83bf-4e23-ac5c-54f1974a98c2"},"source":["optimizer.param_groups[0].keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['lr', 'weight_decay', 'momentum', 'dampening', 'nesterov', 'params'])"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OubOQHJ7OFBQ","executionInfo":{"elapsed":204,"status":"ok","timestamp":1622461395036,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"},"user_tz":-120},"outputId":"a8eb0425-7921-40cb-abba-08a4c18ce931"},"source":["model.device_ids"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0]"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XG3T7_BYzOAl","outputId":"0e9789f2-fe81-446b-dd83-1485550f5dcb"},"source":["train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["max step:  9915\n","https://app.neptune.ai/ch.kalavritinos/OAA/e/OAA-78\n","Session started:  Sun May 30 20:57:24 2021\n","Iter:    0/ 9915 loss:0.7188 0.9207 1.1738 0.3903 cnt:111303 36960 6617 imps:1.1 lr: 0.0100\n","Iter:   50/ 9915 loss:0.6518 0.6876 0.7764 0.5717 cnt:113898 40116 4858 imps:2.6 lr: 0.0100\n","Iter:  100/ 9915 loss:0.6302 0.5974 0.7157 0.6039 cnt:115074 37393 4328 imps:2.6 lr: 0.0099\n","Iter:  150/ 9915 loss:0.6248 0.5729 0.7065 0.6100 cnt:111791 41274 5411 imps:2.7 lr: 0.0099\n","Iter:  200/ 9915 loss:0.6208 0.5637 0.7118 0.6038 cnt:114947 38993 4774 imps:2.6 lr: 0.0098\n","Iter:  250/ 9915 loss:0.6029 0.5304 0.7124 0.5844 cnt:113523 40818 5037 imps:2.6 lr: 0.0098\n","Iter:  300/ 9915 loss:0.5919 0.5291 0.7022 0.5681 cnt:117885 38496 4648 imps:2.6 lr: 0.0097\n","Iter:  350/ 9915 loss:0.5495 0.5101 0.6559 0.5160 cnt:116647 36065 4674 imps:2.6 lr: 0.0097\n","Iter:  400/ 9915 loss:0.5105 0.5048 0.5936 0.4717 cnt:112632 39681 4928 imps:2.6 lr: 0.0096\n","Iter:  450/ 9915 loss:0.5277 0.5012 0.5936 0.5079 cnt:115019 36203 4445 imps:2.6 lr: 0.0096\n","Iter:  500/ 9915 loss:0.4945 0.4841 0.5865 0.4536 cnt:118743 34817 4887 imps:2.6 lr: 0.0095\n","Iter:  550/ 9915 loss:0.4832 0.4436 0.5619 0.4635 cnt:115590 36126 4543 imps:2.6 lr: 0.0095\n","Iter:  600/ 9915 loss:0.4846 0.4597 0.5634 0.4577 cnt:118538 35754 4622 imps:2.6 lr: 0.0095\n","Iter:  650/ 9915 loss:0.4641 0.4569 0.5418 0.4289 cnt:119021 35030 4929 imps:2.6 lr: 0.0094\n","Model saved to checkpoints/train_aff/exp2/voc_epoch_0.pth\n","Iter:  700/ 9915 loss:0.4762 0.4432 0.5498 0.4560 cnt:114567 37293 5109 imps:0.1 lr: 0.0094\n","Iter:  750/ 9915 loss:0.4624 0.4275 0.5401 0.4409 cnt:116463 37876 4679 imps:0.3 lr: 0.0093\n","Iter:  800/ 9915 loss:0.4604 0.4488 0.5136 0.4397 cnt:119943 34741 4592 imps:0.4 lr: 0.0093\n","Iter:  850/ 9915 loss:0.4608 0.4377 0.5278 0.4388 cnt:115193 37114 4637 imps:0.5 lr: 0.0092\n","Iter:  900/ 9915 loss:0.4587 0.4107 0.5281 0.4480 cnt:115163 38822 4535 imps:0.6 lr: 0.0092\n","Iter:  950/ 9915 loss:0.4653 0.4321 0.5374 0.4459 cnt:117261 37877 4636 imps:0.7 lr: 0.0091\n","Iter: 1000/ 9915 loss:0.4535 0.4343 0.5187 0.4304 cnt:119342 38648 4902 imps:0.8 lr: 0.0091\n","Iter: 1050/ 9915 loss:0.4413 0.4399 0.4956 0.4148 cnt:119611 35694 4731 imps:0.8 lr: 0.0090\n","Iter: 1100/ 9915 loss:0.4411 0.4348 0.4750 0.4272 cnt:116550 35779 4757 imps:0.9 lr: 0.0090\n","Iter: 1150/ 9915 loss:0.4321 0.4155 0.4947 0.4091 cnt:115598 38028 4652 imps:0.9 lr: 0.0089\n","Iter: 1200/ 9915 loss:0.4429 0.4211 0.4941 0.4283 cnt:118221 37574 4779 imps:1.0 lr: 0.0089\n","Iter: 1250/ 9915 loss:0.4501 0.4279 0.5084 0.4320 cnt:122153 34314 4807 imps:1.0 lr: 0.0089\n","Iter: 1300/ 9915 loss:0.4253 0.4211 0.4697 0.4051 cnt:113750 38615 5075 imps:1.1 lr: 0.0088\n","Model saved to checkpoints/train_aff/exp2/voc_epoch_1.pth\n","Iter: 1350/ 9915 loss:0.4330 0.4164 0.4936 0.4110 cnt:114089 36661 4808 imps:0.0 lr: 0.0088\n","Iter: 1400/ 9915 loss:0.4338 0.4309 0.5029 0.4007 cnt:118230 36840 4459 imps:0.1 lr: 0.0087\n","Iter: 1450/ 9915 loss:0.4199 0.4059 0.4766 0.3985 cnt:115339 37673 4921 imps:0.2 lr: 0.0087\n","Iter: 1500/ 9915 loss:0.4263 0.4168 0.4826 0.4028 cnt:116904 36159 4587 imps:0.3 lr: 0.0086\n","Iter: 1550/ 9915 loss:0.4094 0.4006 0.4630 0.3870 cnt:113196 36733 4854 imps:0.3 lr: 0.0086\n","Iter: 1600/ 9915 loss:0.4326 0.4161 0.4798 0.4173 cnt:113609 38622 5051 imps:0.4 lr: 0.0085\n","Iter: 1650/ 9915 loss:0.4230 0.4136 0.4938 0.3924 cnt:118279 34456 4298 imps:0.4 lr: 0.0085\n"],"name":"stdout"}]}]}