{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"infer_seam.ipynb","provenance":[],"collapsed_sections":["5m6QvdGfPl_K","iJHhjLUarb6r"],"mount_file_id":"1tgB8enYqqyMCQHSiUWseoOdE_04VJV2y","authorship_tag":"ABX9TyND73otM7e9W4NVf2AAytrq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"K1fhZPM3P06m"},"source":["# Pre-requisites  \n","- download dataset.zip,  \n","- unzip dataset,    \n","- mount drive,  "]},{"cell_type":"code","metadata":{"id":"AFOXQ7FecMwO","executionInfo":{"status":"ok","timestamp":1622135483169,"user_tz":-120,"elapsed":181401,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}}},"source":["# Package installations\n","from IPython.utils import io\n","with io.capture_output() as captured:\n","    !pip install gdown\n","    !pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n","    !pip3 install pillow==6.1\n","\n","    # Install neptune.ai client for monitoring the training process\n","    !pip install neptune-client\n","    # !pip install psutil\n","    !pip install neptune-client neptune-tensorboard\n","    !pip install neptune-contrib\n","    !pip install pillow==4.1.1"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Pv8I7OzfSUN"},"source":["# Download VOC2012\n","# !gdown https://drive.google.com/uc?id=1PDTEuTnWJZNWogxYdqYGOlEZHK8dYET9\n","\n","# Download custom Kvasir-v2\n","# ! gdown https://drive.google.com/uc?id=1WG5F7VVQe6mNupR1LLirkG_Apx6Bup5J"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dFUSDDvsoyiD"},"source":["# with io.capture_output() as captured:\n","#   # !unzip -q VOC2012.zip\n","#   !unzip -q Kvasir-v2.zip\n","#   # !rm -rf VOC2012.zip\n","#   !rm -rf Kvasir-v2.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UhupCLMNOeR6","executionInfo":{"status":"ok","timestamp":1622135664443,"user_tz":-120,"elapsed":4807,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}}},"source":["import neptune\n","from neptunecontrib.monitoring.keras import NeptuneMonitor\n","from IPython.utils import io\n","# connect run to project\n","NEPTUNE_TOKEN = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhMGUxMmQ1NC00ZDU4LTQ4ZGYtOWJjOC0xYTJkYjJmYmJiZDMifQ=='\n","run = neptune.init(project_qualified_name='ch.kalavritinos/SEAM', api_token=NEPTUNE_TOKEN)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfiqv_WzzMs4","executionInfo":{"status":"ok","timestamp":1622135491303,"user_tz":-120,"elapsed":1136,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"4f9ba264-5e9a-4d2c-f31b-db0bdefd2873"},"source":["!git clone https://github.com/YudeWang/SEAM.git"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'SEAM'...\n","remote: Enumerating objects: 50, done.\u001b[K\n","remote: Counting objects: 100% (50/50), done.\u001b[K\n","remote: Compressing objects: 100% (46/46), done.\u001b[K\n","remote: Total 50 (delta 11), reused 30 (delta 4), pack-reused 0\u001b[K\n","Unpacking objects: 100% (50/50), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gjpWrSQYzQHe","executionInfo":{"status":"ok","timestamp":1622135508033,"user_tz":-120,"elapsed":255,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"247e827c-7859-4cb6-b75c-4d93a40612db"},"source":["%cd SEAM"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/SEAM\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8PiwVHzFpZb4","executionInfo":{"status":"ok","timestamp":1622135669187,"user_tz":-120,"elapsed":211,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"d938893d-df31-4011-e3de-79115a36e489"},"source":["with io.capture_output() as captured:\n","  from google.colab import drive \n","  drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/MAI/thesis/source"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MAI/thesis/source\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v8xhcutR0_UA","executionInfo":{"status":"ok","timestamp":1622135802295,"user_tz":-120,"elapsed":232,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"3b8d54f4-5182-48c9-e2aa-21defcb30e64"},"source":["%cd /content/SEAM"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/SEAM\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1A-WOb_p1vSV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OxN8n891tXx"},"source":["python infer_SEAM.py --weights $SEAM_weights --infer_list [voc12/val.txt | voc12/train.txt | voc12/train_aug.txt] --out_cam $your_cam_dir --out_crf $your_crf_dir"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwdc9096kMdG"},"source":["# SEAM infer\n","Imports"]},{"cell_type":"markdown","metadata":{"id":"FzSrMVV1ot0s"},"source":["Change directory to SEAM\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7bpdwddmhpl","executionInfo":{"status":"ok","timestamp":1622135672994,"user_tz":-120,"elapsed":863,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}},"outputId":"fedca37f-13f6-41a5-ccd2-d225f4d4ff6b"},"source":["%cd SEAM/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MAI/thesis/source/SEAM\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n-YtUHpsr7ki","executionInfo":{"status":"ok","timestamp":1622135758676,"user_tz":-120,"elapsed":227,"user":{"displayName":"Charis Kalavritinos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYjFRsI6ZC8nyx8tqHU4floDuoDF1j90tE6vo50A=s64","userId":"18300089374314055480"}}},"source":["import sys\n","sys.path.append('/content/drive/MyDrive/MAI/thesis/source/SEAM')\n","import numpy as np\n","import torch\n","import random\n","import cv2\n","import os\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import voc12.data\n","import kvasirv2.data\n","from utils import pyutils, imutils, torchutils#, visualization\n","import argparse\n","import importlib\n","import torch.nn.functional as F"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLOOrO2I1Zgr"},"source":["# PARAMETERS"]},{"cell_type":"code","metadata":{"id":"DGeIk8ECqAdN"},"source":["root_dir = 'OAA'\n","data_root = '/content/drive/MyDrive/MAI/thesis/source/kvasir-dataset-v2/'   # dataset directory\n","train_list = 'kvasirv2/train.txt'           # list of train images\n","test_list = 'kvasirv2/val.txt'              # list of val images\n","snapshot_dir = 'checkpoints/train/exp3/'    # where to save models\n","att_dir = './results_kvasir/exp3/attention/'   # where to save attentions\n","dataset = 'kvasir-v2'                       # dataset used\n","\n","\n","epoch = 14\n","lr = 0.001\n","batch_size = 32\n","input_size = 256\n","disp_interval = 100\n","num_classes = 20\n","num_workers = 8\n","weight_decay = 0.0005\n","decay_points = '5,10'\n","\n","crop_size = 224         # for cropping images\n","threshold = 0.6         # for probabilities\n","disp_interval = 100     # display interval\n","resume  ='True'        # resume training \n","global_counter = 0      \n","current_epoch = 0       # number of current epoch for resuming"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5m6QvdGfPl_K"},"source":["# data.py  \n","for Kvasir-v2"]},{"cell_type":"code","metadata":{"id":"R57oDOTLDkj7"},"source":["IMG_FOLDER_NAME = \"polyps/\"\n","ANNOT_FOLDER_NAME = \"polyps/masks\"\n","\n","CAT_LIST = ['dyed-lifted-polyp',\n","            'dyed-resection-margins',\n","            'esophagitis',\n","            'normal-cecum',\n","            'normal-pylorus',\n","            'normal-z-line', \n","            'polyps',\n","            'ulcerative-colitis']\n","\n","CAT_NAME_TO_NUM = dict(zip(CAT_LIST,range(len(CAT_LIST))))\n","\n","\n","def load_image_label_list_from_npy(img_name_list):\n","    cls_labels_dict = np.load('kvasirv2/cls_labels.npy', allow_pickle=True).item()\n","    # print(cls_labels_dict)\n","    return [cls_labels_dict[img_name] for img_name in img_name_list]\n","\n","def get_img_path(img_name, dataset_root):\n","    return os.path.join(dataset_root, img_name)\n","\n","def load_img_name_list(dataset_path):\n","    img_gt_name_list = open(dataset_path).read().splitlines()\n","    img_name_list = [img_gt_name.split(' ')[0][-40:-4] for img_gt_name in img_gt_name_list]\n","    folder_paths_list = [img_gt_name.split(' ')[0] for img_gt_name in img_gt_name_list]\n","    return img_name_list, folder_paths_list\n","\n","def load_label_list(dataset_path):\n","    zero_array = np.zeros(8, dtype=np.float32)\n","    list_of_arrays = []\n","    for i in range(8):\n","        z = np.zeros(8, dtype=np.float32)\n","        z[i]=1.0\n","        list_of_arrays.append(z)\n","    #print(list_of_arrays)\n","    img_name_list = open(dataset_path).read().splitlines()\n","    label_list = [list_of_arrays[int(img_name[-1:])-1] for img_name in img_name_list]\n","    return label_list\n","\n","class KvasirImageDataset(Dataset):\n","    def __init__(self, img_name_list_path, dataset_root, transform=None):\n","        self.img_name_list, self.folder_paths_list = load_img_name_list(img_name_list_path)\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_name_list)\n","\n","    def __getitem__(self, idx):\n","        name = self.img_name_list[idx]\n","        path = self.folder_paths_list[idx]\n","        #print(get_img_path(path, self.dataset_root))\n","        img = PIL.Image.open(get_img_path(path, self.dataset_root)).convert(\"RGB\")\n","        # img = torch.from_numpy(np.array(img))\n","\n","        if self.transform:\n","            img = self.transform(img)\n","        return name, img\n","\n","class KvasirClsDataset(KvasirImageDataset):           # inherit init from previous class\n","    def __init__(self, img_name_list_path, dataset_root, transform=None):\n","        super().__init__(img_name_list_path, dataset_root, transform)\n","        self.label_list = load_image_label_list_from_npy(self.img_name_list)        # get list from .npy file\n","        #self.label_list = load_image_label_list_from_xml(self.img_name_list, self.dataset_root)\n","\n","    def __getitem__(self, idx):\n","        name, img = super().__getitem__(idx)\n","        label = torch.from_numpy(self.label_list[idx])\n","        return name, img, label\n","\n","class KvasirClsDatasetMSF(KvasirClsDataset):\n","    def __init__(self, img_name_list_path, data_root, scales, inter_transform=None, unit=1):\n","        super().__init__(img_name_list_path, data_root, transform=None)\n","        self.scales = scales\n","        self.unit = unit\n","        self.inter_transform = inter_transform\n","\n","    def __getitem__(self, idx):\n","        name, img, label = super().__getitem__(idx)\n","        rounded_size = (int(round(img.size[0]/self.unit)*self.unit), int(round(img.size[1]/self.unit)*self.unit))\n","\n","        ms_img_list = []\n","        for s in self.scales:\n","            target_size = (round(rounded_size[0]*s),\n","                           round(rounded_size[1]*s))\n","            s_img = img.resize(target_size, resample=PIL.Image.CUBIC)\n","            ms_img_list.append(s_img)\n","\n","        if self.inter_transform:\n","            for i in range(len(ms_img_list)):\n","                ms_img_list[i] = self.inter_transform(ms_img_list[i])\n","\n","        msf_img_list = []\n","        for i in range(len(ms_img_list)):\n","            msf_img_list.append(ms_img_list[i])\n","            msf_img_list.append(np.flip(ms_img_list[i], -1).copy())\n","        return name, msf_img_list, label\n","\n","class KvasirClsDatasetMS(KvasirClsDataset):\n","    def __init__(self, img_name_list_path, dataset_root, scales, inter_transform=None, unit=1):\n","        super().__init__(img_name_list_path, dataset_root, transform=None)\n","        self.scales = scales\n","        self.unit = unit\n","        self.inter_transform = inter_transform\n","\n","    def __getitem__(self, idx):\n","        name, img, label = super().__getitem__(idx)\n","        rounded_size = (int(round(img.size[0]/self.unit)*self.unit), int(round(img.size[1]/self.unit)*self.unit))\n","        ms_img_list = []\n","        for s in self.scales:\n","            target_size = (round(rounded_size[0]*s),\n","                           round(rounded_size[1]*s))\n","            s_img = img.resize(target_size, resample=PIL.Image.CUBIC)\n","            ms_img_list.append(s_img)\n","\n","        if self.inter_transform:\n","            for i in range(len(ms_img_list)):\n","                ms_img_list[i] = self.inter_transform(ms_img_list[i])\n","\n","        return name, ms_img_list, label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJHhjLUarb6r"},"source":["# Losses"]},{"cell_type":"code","metadata":{"id":"gqGPrPSprep2"},"source":["class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        intersection = (inputs * targets).sum()\n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","        return 1 - dice\n","\n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        intersection = (inputs * targets).sum()\n","        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","        return Dice_BCE\n","\n","class IoULoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(IoULoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        #intersection is equivalent to True Positive count\n","        #union is the mutually inclusive area of all labels & predictions\n","        intersection = (inputs * targets).sum()\n","        total = (inputs + targets).sum()\n","        union = total - intersection\n","        IoU = (intersection + smooth)/(union + smooth)\n","\n","        return -IoU\n","\n","class IoUBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(IoUBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        #intersection is equivalent to True Positive count\n","        #union is the mutually inclusive area of all labels & predictions\n","        intersection = (inputs * targets).sum()\n","        total = (inputs + targets).sum()\n","        union = total - intersection\n","        IoU = - (intersection + smooth)/(union + smooth)\n","\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        IoU_BCE = BCE + IoU\n","\n","        return IoU_BCE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P7Hv-IM4PqZI"},"source":["# train.py"]},{"cell_type":"code","metadata":{"id":"Hot3Bb5YKdve"},"source":["def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):     # save points during training\n","    \"\"\"Function for saving checkpoints\"\"\"\n","    savepath = os.path.join(snapshot_dir, filename)\n","    torch.save(state, savepath)\n","    if is_best:                                                         # if current epoch is best, then save checkpoint\n","        shutil.copyfile(savepath, os.path.join(snapshot_dir, 'model_best.pth.tar'))\n","\n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer, checkpoint['epoch']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Y6pcwDcg_YF"},"source":["def adaptive_min_pooling_loss(x):\n","    # This loss does not affect the highest performance, but change the optimial background score (alpha)\n","    n,c,h,w = x.size()\n","    k = h*w//4\n","    x = torch.max(x, dim=1)[0]\n","    y = torch.topk(x.view(n,-1), k=k, dim=-1, largest=False)[0]\n","    y = F.relu(y, inplace=False)\n","    loss = torch.sum(y)/(k*n)\n","    return loss\n","\n","def max_onehot(x):\n","    n,c,h,w = x.size()\n","    x_max = torch.max(x[:,1:,:,:], dim=1, keepdim=True)[0]\n","    x[:,1:,:,:][x[:,1:,:,:] != x_max] = 0\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"De2vSDoP_iVV"},"source":["class VOCDataset(Dataset):\n","    def __init__(self, datalist_file, root_dir, num_classes=20, transform=None, test=False):\n","        self.root_dir = root_dir\n","        self.testing = test\n","        self.datalist_file =  datalist_file\n","        self.transform = transform\n","        self.num_classes = num_classes\n","        self.image_list, self.label_list = self.read_labeled_image_list(self.root_dir, self.datalist_file)\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        img_name =  self.image_list[idx]\n","        image = PIL.Image.open(img_name).convert('RGB')\n","        image_arr = np.array(image)\n","        # print(image_arr.shape)\n","        \n","        if self.transform is not None:\n","            image = self.transform(image)\n","        # print(\"tranformed img:\",np.array(image).shape)\n","        if self.testing:\n","            return img_name, image, self.label_list[idx]\n","        \n","        return image, self.label_list[idx]\n","\n","    def read_labeled_image_list(self, data_dir, data_list):\n","        # print(data_dir)\n","        with open(data_list, 'r') as f:\n","            lines = f.readlines()\n","        img_name_list = []\n","        img_labels = []\n","        for line in lines:\n","            fields = line.strip().split()\n","            image = fields[0] + '.jpg'\n","            labels = np.zeros((self.num_classes,), dtype=np.float32)\n","            for i in range(len(fields)-1):\n","                index = int(fields[i+1])\n","                labels[index] = 1.\n","            img_name_list.append(os.path.join(data_dir, image))\n","            img_labels.append(labels)\n","        return img_name_list, img_labels#, np.array(img_labels, dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z1tb3vcUvlpq"},"source":["def worker_init_fn(worker_id):\n","        np.random.seed(1 + worker_id)\n","\n","def train(root_dir = 'OAA',\n","          # data_root = '/content/drive/MyDrive/MAI/thesis/source/kvasir-dataset-v2/',\n","          data_root = '/content/drive/MyDrive/MAI/thesis/source/VOCdevkit/',\n","          # train_list = 'kvasirv2/train.txt',\n","          train_list = 'voc12/train_oaa.txt',\n","          # val_list = 'kvasirv2/val.txt',              # list of val images\n","          val_list = 'voc12/val_oaa.txt',              # list of val images\n","          snapshot_dir = 'checkpoints/train/exp4/',    # where to save models\n","          # att_dir = './results_kvasir/exp3/attention/',   # where to save attentions\n","          att_dir = './results_voc/exp4/attention/',   # where to save attentions\n","          dataset = 'voc',                       # dataset used\n","          epoch = 14,\n","          lr = 0.001,\n","          batch_size = 8,\n","          input_size = 256,\n","          disp_interval = 100,\n","          num_classes = 20,\n","          num_workers = 8,\n","          weight_decay = 5e-4,\n","          decay_points = '5,10',\n","          crop_size = 448,         # for cropping images\n","          threshold = 0.6,         # for probabilities\n","          resume  = 'True',        # resume training \n","          global_step = 0,      \n","          current_epoch = 0,\n","          network = \"network.resnet38_SEAM\",\n","          session_name = \"resnet38_SEAM\",\n","          weights = \"\",\n","          tblog_dir = \"./tblog\"):\n","    if not os.path.exists(snapshot_dir):\n","        os.makedirs(snapshot_dir)\n","\n","    model = getattr(importlib.import_module(network), 'Net')() # init model\n","    \n","    # train_dataset = voc12.data.VOC12ClsDataset(train_list, voc12_root=data_root,\n","    #                                            transform=transforms.Compose([\n","    #                     imutils.RandomResizeLong(448, 768),\n","    #                     transforms.RandomHorizontalFlip(),\n","    #                     transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n","    #                     np.asarray,\n","    #                     model.normalize,\n","    #                     imutils.RandomCrop(crop_size),\n","    #                     imutils.HWC_to_CHW,\n","    #                     torch.from_numpy\n","    #                 ]))  # get class for train dataset\n","    # print('before making dataset...')\n","\n","    train_dataset = kvasirv2.data.KvasirClsDataset(train_list, dataset_root=data_root,\n","                                               transform=transforms.Compose([\n","                        imutils.RandomResizeLong(448, 768),\n","                        transforms.RandomHorizontalFlip(),\n","                        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n","                        np.asarray,\n","                        model.normalize,\n","                        imutils.RandomCrop(crop_size),\n","                        imutils.HWC_to_CHW,\n","                        torch.from_numpy\n","                    ]))\n","    # print('after')      # print type of dataset\n","    \n","    # Load train data uing default DataLoader\n","    train_data_loader = DataLoader(train_dataset, batch_size=batch_size,\n","                                   shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True,\n","                                   worker_init_fn=worker_init_fn)\n","    max_step = len(train_dataset) // batch_size * epoch             # calc step\n","\n","    # print('max_step done')\n","    param_groups = model.get_parameter_groups()\n","    optimizer = torchutils.PolyOptimizer([              # use polyoptimizer \n","        {'params': param_groups[0], 'lr': lr, 'weight_decay': weight_decay},\n","        {'params': param_groups[1], 'lr': 2*lr, 'weight_decay': 0},\n","        {'params': param_groups[2], 'lr': 10*lr, 'weight_decay': weight_decay},\n","        {'params': param_groups[3], 'lr': 20*lr, 'weight_decay': 0}\n","    ], lr=lr, weight_decay=weight_decay, max_step=max_step)\n","\n","    if resume:\n","      model, optimizer, current_epoch = load_ckp(snapshot_dir+'voc_epoch_1.pth', model, optimizer)\n","      print('weights loaded!')\n","\n","    if weights[-7:] == '.params':              # if file ends with .params extension import resnet38d\n","        # print('yes .params')\n","        import network.resnet38d\n","        # print(\" resnet38d imported\")\n","        assert 'resnet38' in network\n","        weights_dict = network.resnet38d.convert_mxnet_to_torch(weights)\n","        # print('weights loaded')\n","    else:\n","        weights_dict = torch.load(weights)\n","    # print('weights loaded')\n","    model.load_state_dict(weights_dict, strict=False)           # load state dict\n","    # print('state dict loaded')\n","    model = torch.nn.DataParallel(model).cuda()\n","    # print('parallel model ok')\n","    model.train()\n","    # print('train ok')\n","\n","    avg_meter = pyutils.AverageMeter('loss', 'loss_cls', 'loss_er', 'loss_ecr')         # final loss = l_cls + l_er + l_ecr\n","    timer = pyutils.Timer(\"Session started: \")\n","\n","    best_val_loss = float('inf')\n","    print('Training started ...')\n","\n","    PARAMS = {'dataset':dataset,\n","                'network':'resnet38',\n","                'epoch_nr': epoch,\n","                'batch_size': batch_size,\n","                'optimizer': 'SGD',\n","                'lr': lr\n","          }\n","    neptune.create_experiment('voc12_train', params=PARAMS)\n","\n","    while current_epoch < epoch:\n","        for iter, pack in enumerate(train_data_loader):\n","            scale_factor = 0.3\n","            img1 = pack[1]\n","            img2 = F.interpolate(img1,scale_factor=scale_factor,mode='bilinear',align_corners=True) \n","            N,C,H,W = img1.size()       # get dims\n","            #print(N, C, H, W)\n","            label = pack[2]             # get label\n","            #print(label)\n","            bg_score = torch.ones((N,1))\n","            label = torch.cat((bg_score, label), dim=1)\n","            label = label.cuda(non_blocking=True).unsqueeze(2).unsqueeze(3)\n","            #print(label)\n","            cam1, cam_rv1 = model(img1)\n","            #print(type(cam1), type(cam_rv1))\n","            #print(cam_rv1)\n","\n","            label1 = F.adaptive_avg_pool2d(cam1, (1,1))\n","            loss_rvmin1 = adaptive_min_pooling_loss((cam_rv1*label)[:,1:,:,:])\n","            cam1 = F.interpolate(visualization.max_norm(cam1),scale_factor=scale_factor,mode='bilinear',align_corners=True)*label\n","            cam_rv1 = F.interpolate(visualization.max_norm(cam_rv1),scale_factor=scale_factor,mode='bilinear',align_corners=True)*label\n","\n","            cam2, cam_rv2 = model(img2)\n","            label2 = F.adaptive_avg_pool2d(cam2, (1,1))\n","            loss_rvmin2 = adaptive_min_pooling_loss((cam_rv2*label)[:,1:,:,:])\n","            cam2 = visualization.max_norm(cam2)*label\n","            cam_rv2 = visualization.max_norm(cam_rv2)*label\n","\n","            loss_cls1 = F.multilabel_soft_margin_loss(label1[:,1:,:,:], label[:,1:,:,:])\n","            loss_cls2 = F.multilabel_soft_margin_loss(label2[:,1:,:,:], label[:,1:,:,:])\n","\n","            ns,cs,hs,ws = cam2.size()\n","            loss_er = torch.mean(torch.abs(cam1[:,1:,:,:]-cam2[:,1:,:,:]))\n","\n","            #loss_er = torch.mean(torch.pow(cam1[:,1:,:,:]-cam2[:,1:,:,:], 2))\n","            cam1[:,0,:,:] = 1-torch.max(cam1[:,1:,:,:],dim=1)[0]\n","            cam2[:,0,:,:] = 1-torch.max(cam2[:,1:,:,:],dim=1)[0]\n","#            with torch.no_grad():\n","#                eq_mask = (torch.max(torch.abs(cam1-cam2),dim=1,keepdim=True)[0]<0.7).float()\n","            tensor_ecr1 = torch.abs(max_onehot(cam2.detach()) - cam_rv1)#*eq_mask\n","            tensor_ecr2 = torch.abs(max_onehot(cam1.detach()) - cam_rv2)#*eq_mask\n","            loss_ecr1 = torch.mean(torch.topk(tensor_ecr1.view(ns,-1), k=(int)(2*hs*ws*0.2), dim=-1)[0])\n","            loss_ecr2 = torch.mean(torch.topk(tensor_ecr2.view(ns,-1), k=(int)(2*hs*ws*0.2), dim=-1)[0])\n","            loss_ecr = loss_ecr1 + loss_ecr2\n","\n","            loss_cls = (loss_cls1 + loss_cls2)/2 + (loss_rvmin1 + loss_rvmin2)/2 \n","            loss = loss_cls + loss_er + loss_ecr\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            avg_meter.add({'loss': loss.item(), 'loss_cls': loss_cls.item(), 'loss_er': loss_er.item(), 'loss_ecr': loss_ecr.item()})\n","\n","            if (optimizer.global_step - 1) % 50 == 0:\n","                timer.update_progress(optimizer.global_step / max_step)\n","\n","                print('Iter:%5d/%5d' % (optimizer.global_step-1, max_step),\n","                      'loss:%.4f %.4f %.4f %.4f' % avg_meter.get('loss', 'loss_cls', 'loss_er', 'loss_ecr'),\n","                      'imps:%.1f' % ((iter+1) * batch_size / timer.get_stage_elapsed()),\n","                      'Fin:%s' % (timer.str_est_finish()),\n","                      'lr: %.4f' % (optimizer.param_groups[0]['lr']), flush=True)\n","                neptune.log_metric('train_loss', loss.item())\n","                neptune.log_metric('train_loss_cls', loss_cls.item())\n","                neptune.log_metric('train_loss_er', loss_er.item())\n","                neptune.log_metric('train_loss_ecr', loss_ecr.item())\n","                neptune.log_metric('lr', optimizer.param_groups[0]['lr'])\n","\n","                avg_meter.pop()\n","\n","                # Visualization for training process\n","                img_8 = img1[0].numpy().transpose((1,2,0))\n","                img_8 = np.ascontiguousarray(img_8)\n","                mean = (0.485, 0.456, 0.406)\n","                std = (0.229, 0.224, 0.225)\n","                img_8[:,:,0] = (img_8[:,:,0]*std[0] + mean[0])*255\n","                img_8[:,:,1] = (img_8[:,:,1]*std[1] + mean[1])*255\n","                img_8[:,:,2] = (img_8[:,:,2]*std[2] + mean[2])*255\n","                img_8[img_8 > 255] = 255\n","                img_8[img_8 < 0] = 0\n","                img_8 = img_8.astype(np.uint8)\n","\n","                input_img = img_8.transpose((2,0,1))\n","                neptune.log_image(name = 'input img', filename = input_img)\n","\n","                h = H//4; w = W//4\n","                p1 = F.interpolate(cam1,(h,w),mode='bilinear')[0].detach().cpu().numpy()\n","                p2 = F.interpolate(cam2,(h,w),mode='bilinear')[0].detach().cpu().numpy()\n","                p_rv1 = F.interpolate(cam_rv1,(h,w),mode='bilinear')[0].detach().cpu().numpy()\n","                p_rv2 = F.interpolate(cam_rv2,(h,w),mode='bilinear')[0].detach().cpu().numpy()\n","\n","                image = cv2.resize(img_8, (w,h), interpolation=cv2.INTER_CUBIC).transpose((2,0,1))\n","                CLS1, CAM1, _, _ = visualization.generate_vis(p1, None, image, func_label2color=visualization.VOClabel2colormap, threshold=None, norm=False)\n","                CLS2, CAM2, _, _ = visualization.generate_vis(p2, None, image, func_label2color=visualization.VOClabel2colormap, threshold=None, norm=False)\n","                CLS_RV1, CAM_RV1, _, _ = visualization.generate_vis(p_rv1, None, image, func_label2color=visualization.VOClabel2colormap, threshold=None, norm=False)\n","                CLS_RV2, CAM_RV2, _, _ = visualization.generate_vis(p_rv2, None, image, func_label2color=visualization.VOClabel2colormap, threshold=None, norm=False)\n","                #MASK = eq_mask[0].detach().cpu().numpy().astype(np.uint8)*255\n","                loss_dict = {'loss':loss.item(), \n","                             'loss_cls':loss_cls.item(),\n","                             'loss_er':loss_er.item(),\n","                             'loss_ecr':loss_ecr.item()}\n","                itr = optimizer.global_step - 1\n","                tblogger.add_scalars('loss', loss_dict, itr)\n","                tblogger.add_scalar('lr', optimizer.param_groups[0]['lr'], itr)\n","                tblogger.add_image('Image', input_img, itr)\n","                #tblogger.add_image('Mask', MASK, itr)\n","                tblogger.add_image('CLS1', CLS1, itr)\n","                tblogger.add_image('CLS2', CLS2, itr)\n","                tblogger.add_image('CLS_RV1', CLS_RV1, itr)\n","                tblogger.add_image('CLS_RV2', CLS_RV2, itr)\n","                tblogger.add_images('CAM1', CAM1, itr)\n","                tblogger.add_images('CAM2', CAM2, itr)\n","                tblogger.add_images('CAM_RV1', CAM_RV1, itr)\n","                tblogger.add_images('CAM_RV2', CAM_RV2, itr)\n","\n","        else:\n","            print('')\n","            timer.reset_stage()\n","            save_checkpoint(\n","                          {'epoch': current_epoch,\n","                            'state_dict':model.state_dict(),\n","                            'optimizer':optimizer.state_dict()\n","                          }, is_best=False,\n","                          filename='%s_epoch_%d.pth' %(dataset, current_epoch))\n","\n","    # torch.save(model.module.state_dict(),'pretrained_model/' + session_name + '.pth')           # save final model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2MXMa2NxzRwk"},"source":["# Train the CAM network"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XG3T7_BYzOAl","outputId":"e48759b2-a62c-42a5-9f25-39804a0dfab6"},"source":["train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training started ...\n","https://app.neptune.ai/ch.kalavritinos/OAA/e/OAA-24\n","Epoch: [0][100/2117]\tLR: 0.00100\ttrain loss 0.2427 (0.3020)\t\n","Epoch: [0][200/2117]\tLR: 0.00100\ttrain loss 0.2099 (0.2510)\t\n","Epoch: [0][300/2117]\tLR: 0.00100\ttrain loss 0.1191 (0.2287)\t\n","Epoch: [0][400/2117]\tLR: 0.00100\ttrain loss 0.1343 (0.2107)\t\n","Epoch: [0][500/2117]\tLR: 0.00100\ttrain loss 0.1949 (0.1970)\t\n"],"name":"stdout"}]}]}